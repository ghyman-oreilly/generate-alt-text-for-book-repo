<section data-type="chapter" id="ch02_installing_delta_lake_1726062517888198" xmlns="http://www.w3.org/1999/xhtml">
<h1>Installing Delta Lake</h1>

<p>In <a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-type="indexterm" id="icd201">&nbsp;</a>this <a contenteditable="false" data-primary="installing and setting up Delta Lake" data-type="indexterm" id="kab002">&nbsp;</a>chapter, we will show you how to set up Delta Lake and walk you through the simple steps to start writing your first standalone application.</p>

<p>There are multiple ways you can install Delta Lake. If you are just starting, using a single machine with the Delta Lake Docker image is the best option. If you want to skip the hassle of a local installation, the Databricks Community Edition, which includes the latest version of Delta Lake, is free. Various free trials of Databricks, which natively provides Delta Lake, are also available; check your cloud provider’s documentation for additional details. Other options discussed in this chapter include the Delta Rust Python bindings, the Delta Rust API, and Apache Spark. In this chapter, we also create and verify the Delta Lake tables for illustrative purposes. Delta Lake table creation and other CRUD operations are covered in depth in <a data-type="xref" href="#ch03_essential_delta_lake_operations_1726062519306813">#ch03_essential_delta_lake_operations_1726062519306813</a>.</p>

<section data-type="sect1" id="ch02_delta_lake_docker_image_1726062517888396">
<h1>Delta Lake Docker Image</h1>

<p>The <a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-tertiary="Docker image" data-type="indexterm" id="icd202">&nbsp;</a><a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-type="indexterm" id="icd203">&nbsp;</a>Delta <a contenteditable="false" data-primary="installing and setting up Delta Lake" data-secondary="Docker image" data-type="indexterm" id="kab003">&nbsp;</a>Lake Docker image contains all the necessary components to read and write with Delta Lake, including Python, Rust, PySpark, Apache Spark, and Jupyter Notebooks. The basic prerequisite is having Docker installed on your local machine (you can find installation instructions at <a href="https://oreil.ly/nZTFH">Get Docker</a>). Once you have Docker installed, you can either download the latest prebuilt version of the Delta Lake Docker image from <a href="https://oreil.ly/cG0K_">DockerHub</a> or build the Docker image yourself by following the instructions from the <a href="https://oreil.ly/-BNl0">Delta Lake Docker GitHub repository</a>. Once the image has been built or you have downloaded the correct image, you can then move on to running the quickstart in a notebook or shell. The Docker image is the preferred option to run all the code snippets in this book.</p>

<p class="pagebreak-before">Please note this Docker image comes preinstalled with the following:</p>

<dl>
	<dt>Apache Arrow</dt>
	<dd>
	<p><a href="https://oreil.ly/qbJoo">Apache Arrow</a> is a <a contenteditable="false" data-primary="Apache Arrow" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Arrow, Apache" data-type="indexterm">&nbsp;</a>development platform for in-memory analytics and aims to provide a standardized, language-independent columnar memory format for flat and hierarchical data, as well as libraries and tools for working with this format. It enables fast data processing and movement across different systems and languages, such as C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python, R, Ruby, and Rust.</p>
	</dd>
	<dt>DataFusion</dt>
	<dd>
	<p>Created in 2017 and <a contenteditable="false" data-primary="DataFusion" data-type="indexterm">&nbsp;</a>donated to the Apache Arrow project in 2019, <a href="https://oreil.ly/K6Rv0">DataFusion</a> is a fast, extensible query engine for building high-quality data-centric systems written in Rust that uses the Apache Arrow in-memory format.</p>
	</dd>
	<dt>ROAPI</dt>
	<dd>
	<p><a href="https://oreil.ly/qrCcK">ROAPI</a> is a no-code <a contenteditable="false" data-primary="ROAPI" data-type="indexterm">&nbsp;</a>solution <a contenteditable="false" data-primary="read-only APIs" data-type="indexterm">&nbsp;</a>to automatically spin up read-only APIs for Delta Lake and other sources; it builds on top of Apache Arrow and DataFusion.</p>
	</dd>
	<dt>Rust</dt>
	<dd>
	<p><a href="https://oreil.ly/J7mSl">Rust</a> is a <a contenteditable="false" data-primary="Rust" data-type="indexterm">&nbsp;</a>statically typed, compiled language that offers performance akin to C and C++, but with a focus on safety and memory management. It’s known for its unique ownership model that ensures memory safety without a garbage collector, making it ideal for systems programming in which control over system resources is crucial.</p>
	</dd>
</dl>

<div data-type="note">
<p>We’re using Linux/macOS in this book. If you’re running Windows, you can use Git Bash, WSL, or any shell configured for bash commands. Please refer to the implementation-specific instructions for using other software, such as Docker.</p>
</div>

<p>We will discuss each of the following interfaces in detail, including how to create and read Delta Lake tables with each one:</p>

<ul class="two-col">
	<li>
	<p>Python</p>
	</li>
	<li>
	<p>PySpark Shell</p>
	</li>
	<li>
	<p>JupyterLab Notebook</p>
	</li>
	<li>
	<p>Scala Shell</p>
	</li>
	<li>
	<p>Delta Rust API</p>
	</li>
	<li>
	<p>ROAPI</p>
	</li>
</ul>

<div data-type="note">
<h1>Run Docker Container</h1>

<p>To<a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-tertiary="running containers" data-type="indexterm">&nbsp;</a> start a Docker <a contenteditable="false" data-primary="bash shell, starting Docker container with" data-type="indexterm">&nbsp;</a>container with a bash shell:</p>

<ol>
	<li>
	<p>Open a bash shell.</p>
	</li>
	<li>
	<p>Run the container from the build image with a bash entrypoint using the following command in bash:</p>
	</li>
</ol>

<pre data-type="programlisting">docker run --name delta_quickstart --rm -it \
--entrypoint bash delta_quickstart</pre>
</div>

<section data-type="sect2" id="ch02_delta_lake_for_python_1726062517888470">
<h2>Delta Lake for Python</h2>

<p>First, <a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-tertiary="Python" data-type="indexterm">&nbsp;</a>open <a contenteditable="false" data-primary="Python" data-type="indexterm">&nbsp;</a>a bash shell and run a container from the built image with a bash entrypoint.</p>

<p>Next, use the <code>python3</code> command<a contenteditable="false" data-primary="python3 command" data-type="indexterm">&nbsp;</a> to launch a Python interactive shell session. The following code snippet will create <a contenteditable="false" data-primary="Pandas DataFrame" data-type="indexterm">&nbsp;</a>a <a href="https://oreil.ly/SxmKt">Pandas DataFrame</a>, create a Delta Lake table, generate new data, write by appending new data to this table, and finally read and then show the data from this <a contenteditable="false" data-primary="Delta Lake tables (Delta tables)" data-secondary="creating" data-type="indexterm">&nbsp;</a>Delta Lake table:</p>

<pre data-code-language="python" data-type="programlisting"># Python
import pandas as pd
from deltalake.writer import write_deltalake
from deltalake import DeltaTable

df = pd.DataFrame(range(5), columns=["id"])     # Create Pandas DataFrame
write_deltalake("/tmp/deltars_table", df)       # Write Delta Lake table
df = pd.DataFrame(range(6, 11), columns=["id"]) # Generate new data 
write_deltalake("/tmp/deltars_table", \
        df, mode="append")                      # Append new data
dt = DeltaTable("/tmp/deltars_table")           # Read Delta Lake table
dt.to_pandas()                                  # Show Delta Lake table</pre>

<p>The output should look similar to the following:</p>

<pre data-type="programlisting"># Output
    0
0   0
1   1
... ...
8   9
9  10</pre>

<p class="pagebreak-before">With these Python commands, you have created your first Delta Lake table. You can validate this by reviewing the underlying filesystem that makes up the table. To do that, you can list the contents within the folder of your Delta Lake table that you saved in <em>/tmp/deltars-table</em> by running the <a contenteditable="false" data-primary="ls command" data-type="indexterm">&nbsp;</a>following <code>ls</code> command after you close your Python process:</p>

<pre data-type="programlisting"># Bash
$ ls -lsgA /tmp/deltars_table
total 12
4 -rw-r--r-- 1 NBuser 1610 Apr 13 05:48 0-...-f3c05c4277a2-0.parquet
4 -rw-r--r-- 1 NBuser 1612 Apr 13 05:48 1-...-674ccf40faae-0.parquet
4 drwxr-xr-x 2 NBuser 4096 Apr 13 05:48 _delta_log</pre>

<p>The <em>.parquet</em> files contain the data you see in your Delta Lake table, while the <em>_delta_log</em> contains the Delta table’s transaction log. We will discuss the transaction log in more detail in <a data-type="xref" href="#ch03_essential_delta_lake_operations_1726062519306813">#ch03_essential_delta_lake_operations_1726062519306813</a>.</p>
</section>

<section data-type="sect2" id="ch02_pyspark_shell_1726062517888528">
<h2>PySpark Shell</h2>

<p>First,<a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-tertiary="PySpark shell" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="PySpark" data-secondary="Delta Lake Docker image" data-type="indexterm">&nbsp;</a> open a bash shell and run a container from the built image with a bash entrypoint.</p>

<p>Next, launch a PySpark interactive shell session:</p>

<pre data-type="programlisting"># Bash
$SPARK_HOME/bin/pyspark --packages io.delta:${DELTA_PACKAGE_VERSION} \
--conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
--conf \
"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"</pre>

<p>Let’s run some basic commands in the shell:</p>

<pre data-code-language="python" data-type="programlisting"># Python
# Create a Spark DataFrame
data = spark.range(0, 5)

# Write to a Delta Lake table
(data
   .write
   .format("delta")
   .save("/tmp/delta-table")
)

# Read from the Delta Lake table
df = (spark
        .read
        .format("delta")
        .load("/tmp/delta-table")
        .orderBy("id")
      )

# Show the Delta Lake table
df.show()</pre>

<p>To verify that you have created a Delta Lake table, you can list the contents within your Delta Lake table folder. For example, in the preceding code, you saved the table in <em>/tmp/delta-table</em>. Once you close your <code>pyspark</code> process, run a list command in your Docker shell, and you should see something similar to the following:</p>

<pre class="wrap" data-type="programlisting"># Bash
$ ls -lsgA /tmp/delta-table
total 36
4 drwxr-xr-x 2 NBuser 4096 Apr 13 06:01 _delta_log
4 -rw-r--r-- 1 NBuser  478 Apr 13 06:01 part-00000-56a2c68a-f90e-4764-8bf7-a29a21a04230-c000.snappy.parquet
4 -rw-r--r-- 1 NBuser   12 Apr 13 06:01 .part-00000-56a2c68a-f90e-4764-8bf7-a29a21a04230-c000.snappy.parquet.crc
4 -rw-r--r-- 1 NBuser  478 Apr 13 06:01 part-00001-bcbb45ab-6317-4229-a6e6-80889ee6b957-c000.snappy.parquet
4 -rw-r--r-- 1 NBuser   12 Apr 13 06:01 .part-00001-bcbb45ab-6317-4229-a6e6-80889ee6b957-c000.snappy.parquet.crc
4 -rw-r--r-- 1 NBuser  478 Apr 13 06:01 part-00002-9e0efb76-a0c9-45cf-90d6-0dba912b3c2f-c000.snappy.parquet
4 -rw-r--r-- 1 NBuser   12 Apr 13 06:01 .part-00002-9e0efb76-a0c9-45cf-90d6-0dba912b3c2f-c000.snappy.parquet.crc
4 -rw-r--r-- 1 NBuser  486 Apr 13 06:01 part-00003-909fee02-574a-47ba-9a3b-d531eec7f0d7-c000.snappy.parquet
4 -rw-r--r-- 1 NBuser   12 Apr 13 06:01 .part-00003-909fee02-574a-47ba-9a3b-d531eec7f0d7-c000.snappy.parquet.crc</pre>
</section>

<section data-type="sect2" id="ch02_jupyterlab_notebook_1726062517888584">
<h2>JupyterLab Notebook</h2>

<p>Open<a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-tertiary="JupyterLab notebooks" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="JupyterLab" data-secondary="Delta Lake Docker image" data-type="indexterm">&nbsp;</a> a bash shell and run a container from the built image with a JupyterLab entrypoint:</p>

<pre data-type="programlisting"># Bash
docker run --name delta_quickstart --rm -it \
-p 8888-8889:8888-8889 delta_quickstart</pre>

<p>The command will output a JupyterLab notebook URL. Copy the URL and launch a browser to follow along in the notebook and run each cell.</p>
</section>

<section data-type="sect2" id="ch02_scala_shell_1726062517888638">
<h2>Scala Shell</h2>

<p>First, <a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-tertiary="Scala shell" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Scala" data-secondary="Delta Lake Docker image" data-type="indexterm">&nbsp;</a>open a bash shell and run a container from the built image with a bash entrypoint. Next, launch a Scala interactive shell session:</p>

<pre data-type="programlisting" class="pagebreak-after"># Bash
$SPARK_HOME/bin/spark-shell --packages io.delta:${DELTA_PACKAGE_VERSION} \
--conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
--conf \
"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"</pre>

<p>Let’s run some basic commands in the shell:</p>

<pre data-code-language="scala" data-type="programlisting">// Scala
// Create a Spark DataFrame
val data = spark.range(0, 5)

// Write to a Delta Lake table
(data
   .write
   .format("delta")
   .save("/tmp/delta-table")
)

// Read from the Delta Lake table
val df = (spark
            .read
            .format("delta")
            .load("/tmp/delta-table")
            .orderBy("id")
         )

// Show the Delta Lake table
df.show()</pre>

<p>For instructions on verifying the Delta Lake table, please refer to <a data-type="xref" href="#ch02_pyspark_shell_1726062517888528">#ch02_pyspark_shell_1726062517888528</a>.</p>
</section>

<section data-type="sect2" id="ch02_delta_rust_api_1726062517888692">
<h2>Delta Rust API</h2>

<p>First, <a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-tertiary="Rust API" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Rust" data-secondary="Delta Lake Docker image" data-type="indexterm">&nbsp;</a>open <a contenteditable="false" data-primary="Delta Rust API" data-type="indexterm">&nbsp;</a>a bash shell and run a container from the built image with a bash entrypoint.</p>

<p>Next, execute <em>examples/read_delta_table.rs</em> to <a contenteditable="false" data-primary="--examples read_delta_table.rs command" data-primary-sortas="examples read_delta_table.rs command" data-type="indexterm">&nbsp;</a>review the metadata and files of the <code>covid19_nyt</code> Delta Lake table; this command will list useful output, including the number of files written and their absolute paths, among other information:</p>

<pre data-type="programlisting"># Bash
cd rs
cargo run --example read_delta_table</pre>

<p>Finally, <a contenteditable="false" data-primary="--example read_delta_datafusion command" data-primary-sortas="example read_delta_datafusion" data-type="indexterm">&nbsp;</a>execute <em>examples/read_delta_datafusion.rs</em> to query the <code>covid19_nyt</code> Delta Lake table using DataFusion:</p>

<pre data-type="programlisting"># Bash
cargo run --example read_delta_datafusion</pre>

<p>Running the above command should list the schema and five rows of the data <a contenteditable="false" data-primary="covid19_nyt Delta Lake table" data-secondary="listing schema and data from" data-type="indexterm">&nbsp;</a>from the <code>covid19_nyt</code> Delta Lake table.</p>
</section>

<section data-type="sect2" id="ch02_read_only_apis_roapi_1726062517888747">
<h2>ROAPI</h2>

<p>The <a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-tertiary="ROAPI" data-type="indexterm" id="icd204">&nbsp;</a><a contenteditable="false" data-primary="ROAPI" data-type="indexterm" id="kab004">&nbsp;</a><a contenteditable="false" data-primary="read-only APIs" data-type="indexterm" id="icd204a">&nbsp;</a>rich open ecosystem around Delta Lake enables many novel utilities, such as ROAPI, which is included in the quickstart container. With ROAPI, you can spin up read-only APIs for static Delta Lake datasets without a single line of code. You can query your Delta Lake table with Apache Arrow and DataFusion using ROAPI, which is also preinstalled in the Docker image.</p>

<p>Open a bash shell and run a container from the built image with a bash entrypoint:</p>

<pre data-type="programlisting"># Bash
docker run --name delta_quickstart --rm -it \
-p 8080:8080 --entrypoint bash delta_quickstart</pre>

<p>The API calls are pushed to the <em>nohup.out</em> file. If you haven’t created the <span class="keep-together"><code>deltars_table</code></span> in your <a contenteditable="false" data-primary="deltars_table" data-type="indexterm">&nbsp;</a>container, create it via the option described in the section <a data-type="xref" href="#ch02_delta_lake_for_python_1726062517888470">#ch02_delta_lake_for_python_1726062517888470</a>. Alternatively, you may omit <code>--table 'del&#x2060;tars_table=/tmp/deltars_table/,format=delta'</code> from the command, as well as any steps that call the <code>deltars_table</code>.</p>

<p>Start the ROAPI utility using the <a contenteditable="false" data-primary="nohup command" data-type="indexterm">&nbsp;</a>following <code>nohup</code> command:</p>

<pre data-type="programlisting"># Bash
nohup roapi --addr-http 0.0.0.0:8080 \
--table 'deltars_table=/tmp/deltars_table/,format=delta' \
--table 'covid19_nyt=/opt/spark/work-dir/rs/data/COVID-19_NYT,format=delta' &amp;</pre>

<p>Next, open another shell and connect to the same Docker image:</p>

<pre data-type="programlisting"># Bash
docker exec -it delta_quickstart /bin/bash</pre>

<p>Run the next three steps in the bash shell you launched in the previous step.</p>

<p>Check the schema of the two Delta Lake tables:</p>

<pre data-type="programlisting"># Bash
curl localhost:8080/api/schema</pre>

<p>The output of the preceding command should be along the following lines:</p>

<pre class="wrap" data-type="programlisting"># Output
{
   "covid19_nyt":{"fields":[{"name":"date","data_type":"Utf8","nullable":true,"dict_id":0,"dict_is_ordered":false},{"name":"county","data_type":"Utf8","nullable":true,"dict_id":0,"dict_is_ordered":false},{"name":"state","data_type":"Utf8","nullable":true,"dict_id":0,"dict_is_ordered":false},{"name":"fips","data_type":"Int32","nullable":true,"dict_id":0,"dict_is_ordered":false},{"name":"cases","data_type":"Int32","nullable":true,"dict_id":0,"dict_is_ordered":false},{"name":"deaths","data_type":"Int32","nullable":true,"dict_id":0,"dict_is_ordered":false}]},
   "deltars_table":{"fields":[{"name":"0","data_type":"Int64","nullable":true,"dict_id":0,"dict_is_ordered":false}]}
}</pre>

<p>Query the <code>deltars_table</code>:</p>

<pre data-type="programlisting"># Bash
curl -X POST -d "SELECT * FROM deltars_table"  localhost:8080/api/sql</pre>

<p>The output of the preceding command should be along the following lines:</p>

<pre data-type="programlisting"># Output
[{"0":0},{"0":1},{"0":2},{"0":3},{"0":4},{"0":6},{"0":7},{"0":8},{"0":9},
{"0":10}]</pre>

<p>Query <a contenteditable="false" data-primary="covid19_nyt Delta Lake table" data-secondary="querying" data-type="indexterm">&nbsp;</a>the <code>covid19_nyt</code> Delta Lake table:</p>

<pre data-type="programlisting"># Bash
curl -X POST \
-d "SELECT cases, county, date FROM covid19_nyt ORDER BY cases DESC LIMIT 5" \
localhost:8080/api/sql</pre>

<p>The output of the preceding command should be <a contenteditable="false" data-primary="ROAPI" data-startref="kab004" data-type="indexterm">&nbsp;</a>along <a contenteditable="false" data-primary="installing and setting up Delta Lake" data-secondary="Docker image" data-startref="kab003" data-type="indexterm">&nbsp;</a>the <a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-startref="icd204" data-tertiary="ROAPI" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="read-only APIs" data-startref="icd204a" data-type="indexterm">&nbsp;</a>following <a contenteditable="false" data-primary="Docker" data-secondary="Delta Lake Docker image" data-startref="icd203" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-startref="icd202" data-tertiary="Docker image" data-type="indexterm">&nbsp;</a>lines:</p>

<pre data-type="programlisting"># Output
[
{"cases":1208672,"county":"Los Angeles","date":"2021-03-11"},
{"cases":1207361,"county":"Los Angeles","date":"2021-03-10"},
{"cases":1205924,"county":"Los Angeles","date":"2021-03-09"},
{"cases":1204665,"county":"Los Angeles","date":"2021-03-08"},
{"cases":1203799,"county":"Los Angeles","date":"2021-03-07"}
]</pre>
</section>
</section>

<section data-type="sect1" id="ch02_native_delta_lake_libraries_1726062517888804">
<h1>Native Delta Lake Libraries</h1>

<p>While <a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-tertiary="native libraries" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="libraries, native Delta Lake" id="kab127" data-type="indexterm">&nbsp;</a>Delta Lake’s core functionality is deeply integrated with Apache Spark, the underlying data format and transaction log are designed to be language agnostic. This flexibility has spurred the development of native Delta Lake libraries in various programming languages, offering direct interaction with Delta Lake tables without the overhead of Spark.</p>

<p>These libraries provide lower-level access to Delta Lake’s features, enabling developers to build highly optimized and specialized applications in a language-agnostic way. Developers can choose the language that best suits their needs and expertise. We discuss this in more detail in <a data-type="xref" href="#ch06_building_native_applications_with_delta_lake_1726062526224398">#ch06_building_native_applications_with_delta_lake_1726062526224398</a>.</p>

<section data-type="sect2" id="ch02_multiple_bindings_available_1726062517888858" class="pagebreak-before less_space">
<h2>Multiple Bindings Available</h2>

<p>The Rust <a contenteditable="false" data-primary="native Delta Lake libraries" data-secondary="bindings" data-type="indexterm">&nbsp;</a>library <a contenteditable="false" data-primary="Python" data-secondary="bindings" data-type="indexterm">&nbsp;</a>provides a strong foundation for other non-JVM-based libraries to build pipelines with Delta Lake. The most popular and prominent of those bindings are the Python bindings, which expose a <code>DeltaTable</code> class and optionally integrate seamlessly with Pandas or PyArrow. At the time of this writing, the Delta Lake Python package is compatible with Python versions 3.7 and later and offers many prebuilt <a href="https://oreil.ly/JVvZO">wheels</a> for easy installation on most major operating systems and architectures.</p>

<p>Multiple communities have developed bindings on top of the Rust library, exposing Delta Lake to Ruby, Node, or other C-based connectors. None have yet reached the maturity presently seen in the Python package, partly because none of the other language ecosystems have seen a level of investment in data tooling like that in the Python community. Pandas, Polars, PyArrow, Dask, and more provide a very rich set of tools for developers to read from and write to Delta tables.</p>

<p>More recently, there has been experimental work on a so-called <a href="https://oreil.ly/0P0zv">Delta Kernel initiative</a>, which aims to provide a native Delta Lake library interface for connectors that abstracts away the Delta protocol into one place. This work is still in an early phase but is expected to help consolidate support for native <span class="keep-together">(C/C++,</span> for example) and higher-level engines (e.g., Python or Node) so that everybody can benefit from the more advanced features, such as deletion vectors, by simply upgrading their underlying Delta Kernel versions.</p>
</section>

<section data-type="sect2" id="ch02_installing_the_delta_lake_python_package_1726062517888915">
<h2>Installing the Delta Lake Python Package</h2>

<p>Delta <a contenteditable="false" data-primary="Python" data-startref="kab127" data-secondary="installing package" data-type="indexterm">&nbsp;</a>Lake <a contenteditable="false" data-primary="native Delta Lake libraries" data-secondary="installing Python package" data-type="indexterm">&nbsp;</a>provides native Python bindings based on the <a href="https://oreil.ly/ZtPkY">delta-rs project</a> with <a href="https://oreil.ly/SxmKt">Pandas integration</a>. This Python package can be easily installed with the following <a contenteditable="false" data-primary="pip install delta lake command" data-type="indexterm">&nbsp;</a>command:</p>

<pre data-type="programlisting"># Bash
pip install delta lake</pre>

<p>After installation, you can follow the same steps as outlined in <a data-type="xref" href="#ch02_delta_lake_for_python_1726062517888470">#ch02_delta_lake_for_python_1726062517888470</a>.</p>
</section>
</section>

<section data-type="sect1" id="ch02_apache_spark_with_delta_lake_1726062517888976">
<h1>Apache Spark with Delta Lake</h1>

<p>Apache <a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-tertiary="Apache Spark" data-type="indexterm" id="icd208">&nbsp;</a><a contenteditable="false" data-primary="Apache Spark" data-type="indexterm" id="icd209">&nbsp;</a>Spark <a contenteditable="false" data-primary="Spark" data-see="Apache Spark" data-type="indexterm">&nbsp;</a>is <a contenteditable="false" data-primary="installing and setting up Delta Lake" data-secondary="Apache Spark" data-type="indexterm" id="kab005">&nbsp;</a>an open source engine designed for the processing and analysis of large-scale datasets. It’s architected to be both rapid and versatile and is capable of managing a variety of analytics, both batch and real-time. Spark provides an interface for programming comprehensive clusters, offering implicit data parallelism and fault tolerance. It leverages in-memory computations to enhance speed and data processing over <a contenteditable="false" data-primary="MapReduce operations" data-type="indexterm">&nbsp;</a>MapReduce operations.</p>

<p>Spark offers multilingual support, which allows developers to construct applications in several languages, including Java, Scala, Python, R, and SQL. Spark also incorporates numerous libraries that enable a wide array of data analysis tasks encompassing machine learning, stream processing, and graph analytics.</p>

<p>Spark is written predominantly in Scala, but its APIs are available in Scala, Python, Java, and R. Spark SQL also allows users to write and execute SQL or HiveQL queries. For new users, we recommend exploring the Python API or SQL queries to get started with Apache Spark.</p>

<p>Check out <a class="orm:hideurl" href="https://oreil.ly/G-ZvF"><em>Learning Spark</em></a> (O’Reilly) or <a class="orm:hideurl" href="https://oreil.ly/Yix_E"><em>Spark: The Definitive Guide</em></a> (O’Reilly) <a contenteditable="false" data-primary="Spark: The Definitive Guide (O’Reilly)" data-type="indexterm">&nbsp;</a>for a more detailed introduction to Spark.</p>

<section data-type="sect2" id="ch02_setting_up_delta_lake_with_apache_spark_1726062517889046">
<h2>Setting Up Delta Lake with Apache Spark</h2>

<p>The <a contenteditable="false" data-primary="Apache Spark" data-secondary="setting up Delta Lake with" data-type="indexterm">&nbsp;</a>steps in this section can be executed on your local machine in either of the following ways:</p>

<dl>
	<dt>Interactive execution</dt>
	<dd>
	<p>Start the Spark <a contenteditable="false" data-primary="spark-shell command, Scala" data-type="indexterm">&nbsp;</a>shell (with <code>spark-shell</code> for Scala language, or with <code>pyspark</code> <a contenteditable="false" data-primary="pyspark command" data-type="indexterm">&nbsp;</a>for Python) with Delta Lake and run the code snippets interactively in the shell. In this chapter, we will focus on interactive execution.</p>
	</dd>
	<dt>Run them as a project</dt>
	<dd>
	<p>If, instead of code snippets, you have code in multiple files, you can set up a Maven or <a href="https://oreil.ly/gqTDS">sbt project</a> (Scala or Java) with Delta Lake, with all the source files, and run the project. You could also use the examples provided in the <a href="https://oreil.ly/Qo0mz">GitHub repository</a>.</p>
	</dd>
</dl>

<div data-type="warning">
<p>For all the following instructions, make sure to install the version of Spark <a contenteditable="false" data-primary="Apache Spark" data-secondary="release compatibility matrix" data-type="indexterm">&nbsp;</a>or PySpark<a contenteditable="false" data-primary="PySpark" data-secondary="release compatibility matrix" data-type="indexterm">&nbsp;</a> that is compatible with Delta Lake 2.3.0. See the <a href="https://oreil.ly/4VkQ1">release compatibility matrix</a> for details.</p>
</div>
</section>

<section data-type="sect2" id="ch02_prerequisite_set_up_java_1726062517889128">
<h2>Prerequisite: Set Up Java</h2>

<p>As <a contenteditable="false" data-primary="Apache Spark" data-secondary="setting up Java for" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Java, setting up for Apache Spark" data-type="indexterm">&nbsp;</a>noted in the official Apache Spark <a href="https://oreil.ly/NFI_5">installation instructions</a>, you must ensure that a valid Java version (8, 11, or 17) has been installed and configured correctly on your system using either the system <code>PATH</code> or <a contenteditable="false" data-primary="JAVA_HOME environmental variable" data-type="indexterm">&nbsp;</a>the <code>JAVA_HOME</code> environmental variable.</p>

<p>Readers should make sure to use the Apache Spark version that is compatible with Delta Lake 2.3.0 and above.</p>
</section>

<section data-type="sect2" id="ch02_setting_up_an_interactive_shell_1726062517889186">
<h2>Setting Up an Interactive Shell</h2>

<p>To <a contenteditable="false" data-primary="Apache Spark" data-secondary="setting up interactive shell" data-type="indexterm" id="icd210">&nbsp;</a>use Delta Lake interactively within the Spark SQL, Scala, or Python shells, you need a local installation of Apache Spark. Depending on whether you want to use SQL, Python, or Scala, you can set up either the SQL, PySpark, or Spark shell, respectively.</p>

<section data-type="sect3" id="ch02_spark_sql_shell_1726062517889247">
<h3>Spark SQL shell</h3>

<p>The<a contenteditable="false" data-primary="Apache Spark" data-secondary="setting up interactive shell" data-tertiary="Spark SQL shell" data-type="indexterm">&nbsp;</a> Spark SQL shell, also referred to as the Spark SQL CLI, is an interactive command-line tool designed to facilitate the execution of SQL queries directly from the command line.</p>

<p>Download the <a href="https://oreil.ly/4VkQ1">compatible version of Apache Spark</a> by following instructions in <a contenteditable="false" data-primary="Apache Spark" data-secondary="documentation" data-type="indexterm">&nbsp;</a>the <a href="https://oreil.ly/NFI_5">Spark documentation</a>, either by using pip or by downloading and extracting the archive and <a contenteditable="false" data-primary="spark-sql command" data-type="indexterm">&nbsp;</a>running <code>spark-sql</code> in the extracted directory:</p>

<pre data-type="programlisting"># Bash
bin/spark-sql --packages io.delta:delta-core_2.12:2.3.0 --conf \
"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf \
"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"</pre>

<p>To create your first Delta Lake table, run the following in the Spark SQL shell prompt:</p>

<pre data-code-language="sql" data-type="programlisting">-- SQL
CREATE TABLE delta.`/tmp/delta-table` USING DELTA AS
SELECT col1 AS id FROM VALUES 0, 1, 2, 3, 4;</pre>

<p>You can read back the data written to the table with another simple SQL query:</p>

<pre data-code-language="sql" data-type="programlisting">-- SQL
SELECT * FROM delta.`/tmp/delta-table`;</pre>
</section>

<section data-type="sect3" id="ch02_pyspark_shell_1726062517889299">
<h3>PySpark shell</h3>

<p>The<a contenteditable="false" data-primary="Apache Spark" data-secondary="setting up interactive shell" data-tertiary="PySpark shell" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="PySpark" data-secondary="setting up shell for Apache Spark" data-type="indexterm">&nbsp;</a> PySpark shell, also known as the PySpark CLI, is an interactive environment that facilitates engagement with Spark’s API using the Python programming language. It serves as a platform for learning, testing PySpark examples, and conducting data analysis directly from the command line. The PySpark shell operates as a Read-Eval-Print Loop (REPL), providing a convenient environment for swiftly testing PySpark statements.</p>

<p>Install the PySpark version that is compatible with the Delta Lake version by running the following in the command prompt:</p>

<pre data-type="programlisting"># Bash
pip install pyspark==&lt;<em>compatible-spark-version</em>&gt;</pre>

<p class="pagebreak-before">Next, run PySpark with the Delta Lake package and additional configurations:</p>

<pre data-type="programlisting"># Bash
pyspark --packages io.delta:delta-core_2.12:2.3.0 --conf \
"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf \
"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"</pre>

<p>Finally, to create your first <a contenteditable="false" data-primary="Delta Lake tables (Delta tables)" data-secondary="creating" data-type="indexterm">&nbsp;</a>Delta Lake table, run the following in the PySpark shell prompt:</p>

<pre data-code-language="python" data-type="programlisting"># Python
data = spark.range(0, 5)
data.write.format("delta").save("/tmp/delta-table")</pre>

<p>You can read back the data written to the table with a simple PySpark code snippet:</p>

<pre data-code-language="python" data-type="programlisting"># Python
df = spark.read.format("delta").load("/tmp/delta-table")
df.show()</pre>
</section>

<section data-type="sect3" id="ch02_spark_scala_shell_1726062517889354">
<h3>Spark Scala shell</h3>

<p>The <a contenteditable="false" data-primary="Apache Spark" data-secondary="setting up interactive shell" data-tertiary="Spark Scala shell" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Scala" data-secondary="setting up Spark Scala shell" data-type="indexterm">&nbsp;</a>Spark Scala shell, also referred to as the Spark Scala CLI, is an interactive platform that allows users to interact with Spark’s API using the Scala programming language. It is a potent tool for data analysis and serves as an accessible medium for learning the API.</p>

<p>Download the<a href="https://oreil.ly/4VkQ1"> compatible version of Apache Spark</a> by following instructions in the <a href="https://oreil.ly/HYwom">Spark documentation</a>, either by using pip or by downloading and extracting the archive and running spark-shell in the extracted directory:</p>

<pre data-type="programlisting"># Bash
bin/spark-shell --packages io.delta:delta-core_2.12:2.3.0 --conf \
"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf \
"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"</pre>

<p>To create your first Delta Lake table, run the following in the Scala shell prompt:</p>

<pre data-code-language="scala" data-type="programlisting">// Scala
val data = spark.range(0, 5)
data.write.format("delta").save("/tmp/delta-table")</pre>

<p>You can read back the data written to the table with a <a contenteditable="false" data-primary="installing and setting up Delta Lake" data-secondary="Apache Spark" data-startref="kab005" data-type="indexterm">&nbsp;</a>simple PySpark <a contenteditable="false" data-primary="Apache Spark" data-secondary="setting up interactive shell" data-startref="icd210" data-type="indexterm">&nbsp;</a>​code <a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-startref="icd208" data-tertiary="Apache Spark" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Apache Spark" data-startref="icd209" data-type="indexterm">&nbsp;</a>snippet:</p>

<pre data-code-language="scala" data-type="programlisting" class="pagebreak-after">// Scala
val df = spark.read.format("delta").load("/tmp/delta-table")
df.show()</pre>
</section>
</section>
</section>

<section data-type="sect1" id="ch02_pyspark_declarative_api_1726062517889408" class="less_space">
<h1>PySpark Declarative API</h1>

<p>A <a href="https://oreil.ly/ZPzSi">PyPi package</a> containing <a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-tertiary="PySpark declarative API" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="PySpark" data-secondary="declarative API" data-type="indexterm">&nbsp;</a>the Python APIs for using Delta Lake with Apache Spark is also available. This could be very useful for setting up a Python project and, more importantly, for unit testing. Delta Lake can be installed using the following <a contenteditable="false" data-primary="pip install delta-spark command" data-type="indexterm">&nbsp;</a>command:</p>

<pre data-type="programlisting"># Bash
pip install delta-spark</pre>

<p>And <code>SparkSession</code> <a contenteditable="false" data-primary="SparkSession, configuring" data-type="indexterm">&nbsp;</a>can be configured with the <code>configure_spark_with_delta_pip</code> utility function <a contenteditable="false" data-primary="configure_spark_with_delta_pip utility function" data-type="indexterm">&nbsp;</a>in Delta Lake:</p>

<pre data-code-language="python" data-type="programlisting"># Python
from delta import *
builder = (
  pyspark.sql.SparkSession.builder.appName("MyApp").config(
    "spark.sql.extensions",
    "io.delta.sql.DeltaSparkSessionExtension"
  ).config(
    "spark.sql.catalog.spark_catalog",
    "org.apache.spark.sql.delta.catalog.DeltaCatalog"
  )
)</pre>
</section>

<section data-type="sect1" id="ch02_databricks_community_edition_1726062517889466">
<h1>Databricks Community Edition</h1>

<p>With <a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-tertiary="Databricks Community Edition" data-type="indexterm" id="icd211">&nbsp;</a><a contenteditable="false" data-primary="Databricks Community Edition" data-type="indexterm" id="icd212">&nbsp;</a>the <a href="https://oreil.ly/WnJeY">Databricks Community Edition</a>, Databricks <a contenteditable="false" data-primary="installing and setting up Delta Lake" data-secondary="Databricks Community Edition" data-type="indexterm" id="kab006">&nbsp;</a>has provided a platform for personal use that gives you a cluster of 15 GB memory, which might be just enough to learn Delta Lake with the help of notebooks and the bundled Spark version.</p>

<p>To sign up for Databricks Community Edition, go to the <a href="https://oreil.ly/FccCw">Databricks sign-up page</a>, fill in your details on the form, and click Continue. Choose Community Edition by clicking on the “Get started with Community Edition” link on the second page of the registration form.</p>

<p>After you have successfully created your account, you will be sent an email to verify your email address. After completing the verification, you can log in to Databricks Community Edition to view the Databricks workspace.</p>

<section data-type="sect2" id="ch02_create_a_cluster_with_databricks_runtime_1726062517889527">
<h2>Create a Cluster with Databricks Runtime</h2>

<p class="pagebreak-after">Start <a contenteditable="false" data-primary="Databricks Community Edition" data-secondary="creating clusters" data-type="indexterm" id="icd213">&nbsp;</a>by <a contenteditable="false" data-primary="clusters" data-secondary="creating using Databricks Runtime" data-type="indexterm">&nbsp;</a>clicking on the Compute menu item in the left pane. All the clusters you create will be listed on this page. If this is the first time you are logging in to this account, the page won’t list any clusters since you haven’t yet created any.</p>

<p>Clicking on Create Compute will bring you to a new cluster page. Databricks Runtime 13.3 LTS is, at the time of this writing, selected by default. You can choose any of the latest (preferably LTS) Databricks Runtimes for running the code. For this example, we chose Databricks Runtime 13.3 LTS. For more information on Databricks Runtime releases and the compatibility matrix, please check the <a href="https://oreil.ly/IqhD1">Databricks website</a>.</p>

<p>Next, choose any name you’d like for your cluster; we chose “Delta_Lake_DLDG” (see <a data-type="xref" href="#ch02_figure_1_1726062517872939">#ch02_figure_1_1726062517872939</a>). Then hit the Create Cluster button at top to launch the cluster.</p>

<figure id="ch02_figure_1_1726062517872939"><img alt="A screenshot of a computer

Description automatically generated" src="images/dldg_0201.png" />
<figcaption>Selecting a Databricks Runtime for a new cluster in Databricks Community Edition</figcaption>
</figure>

<div data-type="warning">
<p>You can create only one cluster at a time with Databricks Community Edition. If a cluster already exists, you will need to either use it or delete it before you can create a new cluster.</p>
</div>

<p>Your cluster should be up and running within a few minutes, as shown<a contenteditable="false" data-primary="Databricks Community Edition" data-secondary="creating clusters" data-startref="icd213" data-type="indexterm">&nbsp;</a> in <a data-type="xref" href="#ch02_figure_2_1726062517872971">#ch02_figure_2_1726062517872971</a>.</p>

<figure id="ch02_figure_2_1726062517872971"><img alt="A screenshot of a computer

Description automatically generated" src="images/dldg_0202.png" />
<figcaption>A new cluster up and running</figcaption>
</figure>

<div data-type="note">
<p>Delta Lake is bundled in the Databricks Runtime, so you don’t need to install Delta Lake explicitly either through pip or by using the Maven coordinates of the package to the cluster.</p>
</div>
</section>

<section data-type="sect2" id="ch02_importing_notebooks_1726062517889583">
<h2>Importing Notebooks</h2>

<p>For<a contenteditable="false" data-primary="Databricks Community Edition" data-secondary="importing notebooks" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="JupyterLab" data-secondary="importing notebooks" data-type="indexterm">&nbsp;</a> brevity and ease of understanding, we will use the Jupyter Notebook we saw in the section <a data-type="xref" href="#ch02_jupyterlab_notebook_1726062517888584">#ch02_jupyterlab_notebook_1726062517888584</a>. This notebook is available in the <a href="https://oreil.ly/woPgT">delta-docs GitHub repository</a>. Please copy the notebook link and keep it handy, as you will import the notebook in this step.</p>

<p>Go to Databricks Community Edition and click on Workspace, and then click on the three stacked dots at top right, as shown in <a data-type="xref" href="#ch02_figure_3_1726062517872993">#ch02_figure_3_1726062517872993</a>.</p>

<figure id="ch02_figure_3_1726062517872993"><img alt="A screenshot of a computer

Description automatically generated" src="images/dldg_0203.png" />
<figcaption>Importing a notebook in Databricks Community Edition</figcaption>
</figure>

<p>In the dialog box, click on the URL radio button, paste in the notebook URL, and click Import. This will render the Jupyter Notebook in Databricks Community Edition.</p>
</section>

<section data-type="sect2" id="ch02_attaching_notebooks_1726062517889637">
<h2>Attaching Notebooks</h2>

<p>Now <a contenteditable="false" data-primary="Databricks Community Edition" data-secondary="attaching notebooks" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="JupyterLab" data-secondary="attaching notebooks" data-type="indexterm">&nbsp;</a>select the Delta_Lake_DLDG cluster <a contenteditable="false" data-primary="notebooks" data-see="JupyterLab" data-type="indexterm">&nbsp;</a>you created earlier to run this notebook, as shown in <a data-type="xref" href="#ch02_figure_4_1726062517873041">#ch02_figure_4_1726062517873041</a>.</p>

<figure id="ch02_figure_4_1726062517873041"><img alt="A screenshot of a computer

Description automatically generated" src="images/dldg_0204.png" />
<figcaption>Choosing the cluster you want to attach to the notebook</figcaption>
</figure>

<p>You can now run each cell in the notebook and press Control + Enter on your keyboard to execute the cell<em>.</em> When a Spark Job is running, Databricks Community Edition shows finer details directly in the notebook. You can also navigate to the Spark UI from here.</p>

<p>You will be able to write to and read from the Delta Lake table <a contenteditable="false" data-primary="installing and setting up Delta Lake" data-secondary="Databricks Community Edition" data-startref="kab006" data-type="indexterm">&nbsp;</a>within <a contenteditable="false" data-primary="installing and setting up Delta Lake" data-startref="kab002" data-type="indexterm">&nbsp;</a>this&nbsp;<a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-startref="icd211" data-tertiary="Databricks Community Edition" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Databricks Community Edition" data-startref="icd212" data-type="indexterm">&nbsp;</a>notebook.</p>
</section>
</section>

<section data-type="sect1" id="ch02_conclusion_1726062517889690">
<h1>Conclusion</h1>

<p>In this chapter, we explored the various approaches you can take to get started with Delta Lake, including Delta Docker, Delta Lake for Python, Apache Spark with Delta Lake, PySpark Declarative API, and finally Databricks Community Edition. We showed how easily you can run a simple notebook or a command shell to write to and read from Delta Lake tables. The next chapter will cover writing and reading operations in more detail.</p>

<p>Finally, we showed you how to use any of these approaches to install Delta Lake and the many different ways in which Delta Lake is available. You also learned how to use SQL, Python, Scala, Java, and Rust programming languages through the API to access Delta Lake tables. In the next chapter, we’ll cover the essential operations you need to know to use Delta <a contenteditable="false" data-primary="Delta Lake" data-secondary="installing and setting up" data-startref="icd201" data-type="indexterm">&nbsp;</a>Lake.</p>
</section>
</section>
