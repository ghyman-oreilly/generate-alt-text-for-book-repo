<section class="pagenumrestart" data-type="chapter" id="ch01_introduction_to_the_delta_lake_lakehouse_format_1726062516600606" xmlns="http://www.w3.org/1999/xhtml">
<h1>Introduction to the Delta Lake <span class="keep-together">Lakehouse Format</span></h1>

<p>This <a contenteditable="false" data-primary="Delta Lake" data-type="indexterm" id="icd101">&nbsp;</a>chapter explains Delta Lake’s origins and how it was initially designed to address data integrity issues around petabyte-scale systems. If you are familiar with Delta Lake’s history and instead want to dive into what Delta Lake is, its anatomy, and the Delta transaction protocol, feel free to jump ahead to the section <a data-type="xref" href="#ch01_what_is_delta_lake_1726062516601658">#ch01_what_is_delta_lake_1726062516601658</a> later in this chapter.</p>

<section data-type="sect1" id="ch01_the_genesis_of_delta_lake_1726062516601078">
<h1>The Genesis of Delta Lake</h1>

<p>In<a contenteditable="false" data-primary="Delta Lake" data-secondary="origins of" data-type="indexterm" id="icd102">&nbsp;</a> this section, we’ll chart the course of Delta Lake’s short evolutionary history: its genesis and inspiration, and its adoption in the community as a lakehouse format, ensuring the integrity of every enterprise’s most important asset: its data. The Delta Lake lakehouse format was developed to address the limitations of traditional data lakes and data warehouses. It provides <a href="https://oreil.ly/gjn55">ACID (atomicity, consistency, isolation, and durability) transactions</a> and scalable metadata handling and unifies various data analytics tasks, such as batch and streaming workloads, machine learning, and SQL, on a single platform.</p>

<section data-type="sect2" id="ch01_data_warehousing_data_lakes_and_data_lakehouses_1726062516601158">
<h2>Data Warehousing, Data Lakes, and Data Lakehouses</h2>

<p>There have been many technological advancements in data systems (high-performance computing [HPC] and object databases, for example); a simplified overview of the advancements in querying and aggregating large amounts of business data systems over the last few decades would cover data warehousing, data lakes, and lakehouses. Overall, these systems address online analytics processing<strong> (</strong>OLAP) workloads.</p>

<section data-type="sect3" id="ch01_data_warehousing_1726062516601219">
<h3>Data warehousing</h3>

<p>Data<a contenteditable="false" data-primary="Delta Lake" data-secondary="origins of" data-tertiary="data warehouses" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="data warehouses" data-secondary="overview of" data-type="indexterm">&nbsp;</a> warehouses are purpose-built to aggregate and process large amounts of structured data quickly (<a data-type="xref" href="#ch01_figure_1_1726062516569630">#ch01_figure_1_1726062516569630</a>). To protect this data, they typically use relational databases to provide <a contenteditable="false" data-primary="ACID (atomicity, consistency, isolation, and durability) transactions" data-type="indexterm">&nbsp;</a>ACID <a contenteditable="false" data-primary="atomicity, consistency, isolation, and durability transactions" data-see="ACID transactions" data-type="indexterm">&nbsp;</a>transactions, a step that is crucial for ensuring data integrity for business applications.</p>

<figure id="ch01_figure_1_1726062516569630"><img alt="A diagram of data storage

Description automatically generated" src="images/dldg_0101.png" />
<figcaption>Data warehouses are purpose-built for querying and aggregating structured data</figcaption>
</figure>

<p>Building on the foundation of ACID transactions, data warehouses include management features (backup and recovery controls, gated controls, etc.)&nbsp;to simplify the database operations as well as performance optimizations (indexes, partitioning, etc.) to provide reliable results to the end user more quickly. While robust, <a contenteditable="false" data-primary="data warehouses" data-secondary="limitations in scaling for big data scenarios" data-type="indexterm">&nbsp;</a>data warehouses are often hard to scale to handle the large volumes, variety of analytics (including event processing and data sciences), and data velocity typical in <a contenteditable="false" data-primary="big data, limitations of data warehouses in scaling for" data-type="indexterm">&nbsp;</a>big data scenarios. This limitation is a critical factor that often necessitates using more scalable solutions such as data lakes or distributed processing frameworks like Apache Spark.</p>
</section>

<section data-type="sect3" id="ch01_data_lakes_1726062516601276">
<h3>Data lakes</h3>

<p>Data<a contenteditable="false" data-primary="Delta Lake" data-secondary="origins of" data-tertiary="data lakes" data-type="indexterm" id="icd103">&nbsp;</a><a contenteditable="false" data-primary="data lakes" data-secondary="overview of" data-type="indexterm" id="icd104">&nbsp;</a> lakes are scalable storage repositories (HDFS, cloud object stores such as Amazon S3, ADLS Gen2, and GCS, and so on) that hold vast amounts of raw data in their native format until needed (see <a data-type="xref" href="#ch01_figure_2_1726062516569694">#ch01_figure_2_1726062516569694</a>). Unlike traditional databases, data lakes are designed to handle an internet-scale volume, velocity, and variety of data (e.g., structured, semistructured, and unstructured data). These attributes are commonly associated with big data. Data lakes changed how we store and query large amounts of data because they are designed to scale out the workload across multiple machines or nodes. They are file-based systems that work on clusters of commodity hardware. Traditionally, data warehouses were scaled up on a single machine; note that massively parallel processing data warehouses have existed for quite some time but were more expensive and complex to maintain. Also, while <a contenteditable="false" data-primary="data warehouses" data-secondary="benefits of data lakes over" data-type="indexterm">&nbsp;</a>data warehouses were designed for structured (or tabular) data, data lakes can hold data in the format of one’s choosing, providing developers with flexibility for their data storage.</p>

<figure id="ch01_figure_2_1726062516569694"><img src="images/dldg_0102.png" />
<figcaption />
</figure>

<p>While data lakes could handle all your data for data science and machine learning, they are an inherently unreliable form of data storage. Instead of providing ACID protections, these systems follow the <a contenteditable="false" data-primary="BASE (basically available, soft-state, and eventually consistent) model" data-type="indexterm">&nbsp;</a>BASE model—basically available, soft-state, and <a href="https://oreil.ly/eAqrW">eventually consistent</a>. The lack of <a contenteditable="false" data-primary="ACID (atomicity, consistency, isolation, and durability) transactions" data-type="indexterm">&nbsp;</a>ACID guarantees means the storage system processing failures leave your storage in an inconsistent state with orphaned files. Subsequent queries to the storage system include files that should not result in duplicate counts (i.e., wrong answers).</p>

<p>Together, these <a contenteditable="false" data-primary="data lakes" data-secondary="shortcomings of" data-type="indexterm">&nbsp;</a>shortcomings can lead to an infrastructure poorly suited for BI queries, inconsistent and slow performance, and quite complex setups. Often, the creation of data lakes leads to unreliable data swamps instead of clean data repositories due to the lack of transaction protections, schema management, and<a contenteditable="false" data-primary="data lakes" data-secondary="overview of" data-startref="icd104" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Delta Lake" data-secondary="origins of" data-startref="icd103" data-tertiary="data lakes" data-type="indexterm">&nbsp;</a> so on.</p>
</section>

<section data-type="sect3" id="ch01_lakehouses_or_data_lakehouses_1726062516601476">
<h3>Lakehouses (or data lakehouses)</h3>

<p>The<a contenteditable="false" data-primary="Delta Lake" data-secondary="origins of" data-tertiary="lakehouses" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="lakehouses (data lakehouses)" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="data lakehouses" data-see="lakehouses" data-type="indexterm">&nbsp;</a> lakehouse combines the best elements of data lakes and data warehouses for OLAP workloads. It merges the scalability and flexibility of data lakes with the management features and performance optimization of data warehouses (see <a data-type="xref" href="#ch01_figure_3_1726062516569722">#ch01_figure_3_1726062516569722</a>). There were previous attempts to allow data warehouses and data lakes to coexist side by side. But such an approach was expensive, introducing management complexities, duplication of data, and the reconciliation of reporting/analytics/data science between separate systems. As the practice of data engineering evolved, the concept of the <em>lakehouse</em> was born. A lakehouse eliminates the need for disjointed systems and provides a single, coherent platform for all forms of data analysis. Lakehouses enhance the performance of data queries and simplify data management, making it easier for organizations to derive insights from their data.</p>

<figure id="ch01_figure_3_1726062516569722"><img alt="" src="images/dldg_0103.png" />
<figcaption>Lakehouses are the best of both worlds between data warehouses and data lakes</figcaption>
</figure>

<p>Delta Lake, Apache Iceberg, and Apache Hudi are the most popular open source lakehouse formats. As you can guess, this book will focus on Delta Lake.<span data-type="footnote">To learn more about lakehouses, see the 2021 CIDR whitepaper <a href="https://oreil.ly/ObS0C">“Lakehouse: A New Generation of Open Platforms That Unify Data Warehousing and Advanced Analytics”</a>.</span></p>
</section>
</section>

<section data-type="sect2" id="ch01_project_tahoe_to_delta_lake_the_early_years_month_1726062516601584">
<h2>Project Tahoe to Delta Lake: The Early <span class="strikeout">Years</span> Months</h2>

<p>The <a contenteditable="false" data-primary="Delta Lake" data-secondary="origins of" data-tertiary="name change" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Project Tahoe" data-type="indexterm">&nbsp;</a>2021 online <a contenteditable="false" data-primary=" From Tahoe to Delta Lake online meetup" data-type="indexterm">&nbsp;</a>meetup <a href="https://oreil.ly/52sIg">From Tahoe to Delta Lake</a> provided a nostalgic look back at how Delta Lake was created. The panel featured “old school” developers and Delta Lake maintainers <a contenteditable="false" data-primary="Yavuz, Burak" data-type="indexterm">&nbsp;</a>Burak Yavuz, <a contenteditable="false" data-primary="Lee, Denny" data-type="indexterm">&nbsp;</a>Denny Lee, <a contenteditable="false" data-primary="Zhu, Ryan" data-type="indexterm">&nbsp;</a>Ryan Zhu, and <a contenteditable="false" data-primary="Das, Tathagata" data-type="indexterm">&nbsp;</a>Tathagata Das, as well as the creator of Delta Lake, <a contenteditable="false" data-primary="Armbrust, Michael" data-type="indexterm">&nbsp;</a>Michael Armbrust. It also included the “new school” Delta Lake maintainers who created the delta-rs project, <a contenteditable="false" data-primary="QP Hou" data-type="indexterm">&nbsp;</a>QP Hou and <a contenteditable="false" data-primary="Croy, R. Tyler" data-type="indexterm">&nbsp;</a>R. Tyler Croy.</p>

<p>The original project name for Delta Lake was “Project Tahoe,” as Michael Armbrust had the initial idea of providing transactional reliability for data lakes while skiing at Tahoe in 2017. Lake Tahoe is an iconic and massive lake in California, symbolizing the large-scale data lake the project aimed to create. Michael is a committer/PMC member of Apache Spark™; a Delta Lake maintainer; one of the original creators of Spark SQL, Structured Streaming, and Delta Lake; and a distinguished software engineer at Databricks. The transition from “Tahoe” to “Delta Lake” occurred around New Year’s 2018 and came from <a contenteditable="false" data-primary="Damji, Jules" data-type="indexterm">&nbsp;</a>Jules Damji. The rationale behind changing the name was to invoke the natural process in which rivers flow into deltas, depositing sediments that eventually build up and create fertile ground for crops. This metaphor was fitting for the project, as it represented the convergence of data streams into a managed data lake, where data practitioners could cultivate valuable insights. The <a contenteditable="false" data-primary="Delta Lake" data-secondary="choice of name for" data-type="indexterm">&nbsp;</a>Delta name also resonated with the project’s architecture, which was designed to handle massive and high-velocity data streams, allowing the data to be processed and split into different streams or views.</p>

<p>But why did Armbrust create <a contenteditable="false" data-primary="Delta Lake" data-secondary="motive for creating" data-type="indexterm">&nbsp;</a>Delta Lake? He created it to address the limitations of Apache Spark’s file synchronization. Specifically, he wanted to handle large-scale data operations and needed robust transactional support. Thus, his motivation for developing Delta Lake stemmed from the need for a scalable transaction log that could handle massive data volumes and complex operations.</p>

<p>Early in the creation of <a contenteditable="false" data-primary="Delta Lake" data-secondary="early use cases" data-type="indexterm">&nbsp;</a>Delta Lake are two notable use cases that emphasize its efficiency and scalability. <a contenteditable="false" data-primary="Comcast" data-type="indexterm">&nbsp;</a>Comcast utilized Delta Lake to enhance its data analytics and machine learning platforms and manage its petabytes of data. This transition reduced its compute utilization from 640VMs to 64VMs and simplified job maintenance from 84 to 3 jobs. By streamlining its processing with Delta Lake, Comcast reduced its compute utilization by 10x, with 28x fewer jobs. <a contenteditable="false" data-primary="Apple’s information security team" data-type="indexterm">&nbsp;</a>Apple’s information security team employed Delta Lake for real-time threat detection and response, handling over 300 billion events per day and writing hundreds of terabytes of data daily. Both cases illustrate Delta Lake's superior performance and cost-effectiveness compared to traditional data management methods. We will look at additional use cases<a contenteditable="false" data-primary="Delta Lake" data-secondary="origins of" data-startref="icd102" data-type="indexterm">&nbsp;</a> in <a data-type="xref" href="#ch11_successful_design_patterns_1726062532247214">#ch11_successful_design_patterns_1726062532247214</a>.</p>
</section>
</section>

<section data-type="sect1" id="ch01_what_is_delta_lake_1726062516601658">
<h1>What Is Delta Lake?</h1>

<p>Delta<a contenteditable="false" data-primary="Delta Lake" data-secondary="defined" data-type="indexterm">&nbsp;</a> Lake is an open source storage layer that supports ACID transactions, scalable metadata handling, and unification of streaming and batch data processing. It was initially designed to work with Apache Spark and large-scale data lake workloads.</p>

<p>With Delta Lake, you can build a single data platform with your choice of high-performance query engine to address a diverse range of <a contenteditable="false" data-primary="Delta Lake" data-secondary="workloads addressed by" data-type="indexterm">&nbsp;</a>workloads, <a contenteditable="false" data-primary="workloads" data-type="indexterm">&nbsp;</a>including (but not limited to) business intelligence (BI), streaming analytics/complex event processing, data science, and machine learning, as noted in <a data-type="xref" href="#ch01_figure_4_1726062516569747">#ch01_figure_4_1726062516569747</a>.</p>

<figure id="ch01_figure_4_1726062516569747"><img src="images/dldg_0104.png" />
<figcaption>Delta Lake provides a scalable, open, general-purpose transactional data format for your lakehouse</figcaption>
</figure>

<p>However, as it has evolved, Delta Lake has been optimally designed to work with numerous workloads (small data, medium data, big data, etc.). It has also been designed to work with multiple <a contenteditable="false" data-primary="frameworks" data-type="indexterm">&nbsp;</a>frameworks (e.g., Apache Spark, Apache Flink, Trino, Presto, Apache Hive, and Apache Druid), services (e.g., Athena, Big Query, Databricks, EMR, Fabric, Glue, Starburst, and Snowflake), and languages (.NET, Java, Python, Rust, Scala, SQL, etc.).</p>

<section data-type="sect2" id="ch01_common_use_cases_1726062516601723">
<h2>Common Use Cases</h2>

<p>Developers<a contenteditable="false" data-primary="Delta Lake" data-secondary="use cases" data-type="indexterm">&nbsp;</a> in <a contenteditable="false" data-primary="use cases" data-type="indexterm">&nbsp;</a>all types of organizations, from startups to large enterprises, use Delta Lake to manage their big data and AI workloads. Common use cases include:</p>

<dl>
	<dt>Modernizing data lakes<strong> </strong></dt>
	<dd>
	<p>Delta Lake helps <a contenteditable="false" data-primary="data lakes" data-secondary="modernizing" data-type="indexterm">&nbsp;</a>organizations modernize their data lakes by providing ACID transactions, scalable metadata handling, and schema enforcement, thereby ensuring data reliability and performance improvements.</p>
	</dd>
	<dt>Data warehousing</dt>
	<dd>
	<p>There <a contenteditable="false" data-primary="data warehouses" data-type="indexterm">&nbsp;</a>are both data warehousing technologies and <em>techniques</em>. The Delta Lake lakehouse format allows you to apply data warehousing techniques to provide fast query performance for various analytics workloads while also providing data reliability.</p>
	</dd>
	<dt>Machine learning/data science<strong> </strong></dt>
	<dd>
	<p>Delta Lake <a contenteditable="false" data-primary="machine learning" data-type="indexterm">&nbsp;</a>provides <a contenteditable="false" data-primary="data science" data-type="indexterm">&nbsp;</a>a reliable data foundation for machine learning and data science teams to access and process data, enabling them to build and deploy models faster.</p>
	</dd>
	<dt>Streaming data processing<strong> </strong></dt>
	<dd>
	<p>Delta Lake unifies <a contenteditable="false" data-primary="streaming" data-type="indexterm">&nbsp;</a>streaming and <a contenteditable="false" data-primary="batch processing" data-type="indexterm">&nbsp;</a>batch data processing. This allows developers to process real-time data and perform complex transformations on the fly.</p>
	</dd>
	<dt>Data engineering<strong> </strong></dt>
	<dd>
	<p>Delta Lake provides a reliable and performant platform for <a contenteditable="false" data-primary="data engineering" data-type="indexterm">&nbsp;</a>data engineering teams to build and manage data pipelines, ensuring data quality and accuracy.</p>
	</dd>
	<dt>Business intelligence<strong> </strong></dt>
	<dd>
	<p>Delta Lake supports <a contenteditable="false" data-primary="SQL queries" data-type="indexterm">&nbsp;</a>SQL queries, <a contenteditable="false" data-primary="business intelligence" data-type="indexterm">&nbsp;</a>making it easy for business users to access and analyze data and thus enabling them to make data-driven decisions.</p>
	</dd>
</dl>

<p>Overall, Delta Lake is used by various teams, including data engineers, data scientists, and business users, to manage and analyze big data and AI workloads, ensuring data reliability, performance, and scalability.</p>
</section>

<section data-type="sect2" id="ch01_key_features_1726062516601786" class="pagebreak-before less_space">
<h2>Key Features</h2>

<p>Delta <a contenteditable="false" data-primary="Delta Lake" data-secondary="key features of" data-type="indexterm" id="icd105">&nbsp;</a>Lake comprises the following key features that are fundamental to an open lakehouse format (please see the VLDB research article <a href="https://oreil.ly/PaRNO">“Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores”</a> for a deeper dive into these features):</p>

<dl>
	<dt>ACID transactions</dt>
	<dd>
	<p>Delta Lake ensures that data modifications are performed atomically, consistently, in isolation, and durably, i.e., with <a contenteditable="false" data-primary="ACID (atomicity, consistency, isolation, and durability) transactions" data-type="indexterm">&nbsp;</a>ACID transaction protections. This means that when multiple concurrent clients or tasks access the data, the system maintains data integrity. For instance, if a process fails during a data modification, Delta Lake will roll back the changes, ensuring that the data remains consistent.</p>
	</dd>
	<dt>Scalable metadata</dt>
	<dd>
	<p>The metadata of a <a contenteditable="false" data-primary="scalable metadata" data-type="indexterm">&nbsp;</a>Delta <a contenteditable="false" data-primary="metadata" data-secondary="scalable" data-type="indexterm">&nbsp;</a>Lake table is the transaction log, which provides transactional consistency per the aforementioned ACID transactions. With a petabyte-scale table, the table’s metadata can itself be exceedingly complicated to maintain. Delta Lake’s scalable metadata handling feature is designed to manage metadata efficiently for large-scale datasets without its operations impacting query or processing performance.</p>
	</dd>
	<dt>Time travel</dt>
	<dd>
	<p>The Delta Lake <a contenteditable="false" data-primary="time travel feature" data-type="indexterm">&nbsp;</a>time travel feature allows you to query previous versions of a table to access historical data. Made possible by the Delta transaction log, it enables you to specify a version or timestamp to query a specific version of the data. This is very useful for a variety of use cases, such as data audits, regulatory compliance, and data recovery.</p>
	</dd>
	<dt>Unified batch/streaming</dt>
	<dd>
	<p>Delta Lake was designed hand in <a contenteditable="false" data-primary="unified batch/streaming" data-type="indexterm">&nbsp;</a>hand with <a contenteditable="false" data-primary="Apache Spark Structured Streaming" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Spark Structured Streaming, Apache" data-type="indexterm">&nbsp;</a>Apache <a contenteditable="false" data-primary="Structured Streaming" data-type="indexterm">&nbsp;</a>Spark Structured Streaming to simplify the logic around streaming. Instead of having different APIs for batch and streaming, Structured Streaming uses the same in-memory Datasets/DataFrame API for both scenarios. This allows developers to use the same business logic and APIs, the only difference being latency. Delta Lake provides the ACID guarantees of the storage system to support this unification.</p>
	</dd>
	<dt>Schema evolution/enforcement</dt>
	<dd>
	<p>Delta Lake’s schema <a contenteditable="false" data-primary="schema" data-secondary="evolution of" data-type="indexterm">&nbsp;</a>evolution and schema enforcement ensure data consistency and quality by enforcing a schema on write operations and allowing users to modify the schema without breaking existing queries. They also prevent developers from inadvertently inserting data with incorrect columns or types, which is crucial for maintaining data quality and consistency.</p>
	</dd>
	<dt>Audit history</dt>
	<dd>
	<p>This feature <a contenteditable="false" data-primary="audit history" data-type="indexterm">&nbsp;</a>provides detailed logs of all changes made to the data, including information about who made each change, what the change was, and when it was made. This is crucial for compliance and regulatory requirements, as it allows users to track changes to the data over time and ensure that data modifications are performed correctly. The Delta transaction log makes all of this possible.</p>
	</dd>
	<dt>DML operations</dt>
	<dd>
	<p>Delta Lake was one of the first lakehouse formats to provide <a contenteditable="false" data-primary="data manipulation language (DML)" data-type="indexterm">&nbsp;</a>data manipulation <a contenteditable="false" data-primary="DML (data manipulation language)" data-type="indexterm">&nbsp;</a>language (DML) operations. This initially extended Apache Spark to support various operations such as insert, update, delete, and merge (or CRUD operations). Today, users can effectively modify the data using multiple frameworks, services, and languages.</p>
	</dd>
	<dt>Open source</dt>
	<dd>
	<p>The roots of Delta Lake <a contenteditable="false" data-primary="open source code" data-type="indexterm">&nbsp;</a>were built within the foundation of <a contenteditable="false" data-primary="Databricks" data-type="indexterm">&nbsp;</a>Databricks, which has extensive experience in open source (the founders of Databricks were the original creators of Apache Spark). Shortly after its inception, Delta Lake was donated to the <a contenteditable="false" data-primary="Linux Foundation" data-type="indexterm">&nbsp;</a>Linux Foundation to ensure developers have the ability to use, modify, and distribute the software freely while also promoting collaboration and innovation within the data engineering community.</p>
	</dd>
	<dt>Performance</dt>
	<dd>
	<p>While Delta Lake is a lakehouse storage format, it is optimally designed to improve the speed of your queries and processing for both ingestion and querying using the default configuration. While you can continually tweak the performance of Delta Lake, most of the time the defaults will work for your scenarios.</p>
	</dd>
	<dt>Ease of use</dt>
	<dd>
	<p>Delta Lake was built with simplicity in mind right from the beginning. For example, to write a table using Apache Spark in Parquet file format, you would execute:</p>

	<pre data-code-language="python" data-type="programlisting">data.write.format("parquet").save("/tmp/parquet-table")</pre>

	<p>To do the same thing for Delta, you would <a contenteditable="false" data-primary="Delta Lake" data-secondary="key features of" data-startref="icd105" data-type="indexterm">&nbsp;</a>execute:</p>

	<pre data-code-language="python" data-type="programlisting">data.write.format("delta").save("/tmp/delta-table")</pre>
	</dd>
</dl>
</section>
</section>

<section data-type="sect1" id="ch01_anatomy_of_a_delta_lake_table_1726062516601853" class="pagebreak-before less_space">
<h1>Anatomy of a Delta Lake Table</h1>

<p>A <a contenteditable="false" data-primary="Delta Lake tables (Delta tables)" data-secondary="anatomy of" data-type="indexterm" id="icd106">&nbsp;</a>Delta Lake table or Delta table comprises several key components that work together to provide a robust, scalable, and efficient data storage solution. The main elements are as follows:</p>

<dl>
	<dt>Data files</dt>
	<dd>
	<p>Delta Lake tables <a contenteditable="false" data-primary="data files" data-type="indexterm">&nbsp;</a>store data in <a contenteditable="false" data-primary="Parquet file format" data-type="indexterm">&nbsp;</a>Parquet file format. These files contain the actual data and are stored in a distributed cloud or on-premises file storage system such as HDFS (Hadoop Distributed File System), Amazon S3, Azure Blob Storage (or Azure Data Lake Storage [ADLS] Gen2), GCS (Google Cloud Storage), or MinIO. Parquet was chosen for its efficiency in storing and querying large datasets.</p>
	</dd>
	<dt>Transaction log</dt>
	<dd>
	<p>The transaction log, <a contenteditable="false" data-primary="transaction log" data-type="indexterm">&nbsp;</a>also known as the <a contenteditable="false" data-primary="Delta log" data-see="transaction log" data-type="indexterm">&nbsp;</a>Delta log, is a critical component of Delta Lake. It is an ordered record of every transaction performed on a Delta Lake table. The transaction log ensures ACID properties by recording all changes to the table in a series of <a contenteditable="false" data-primary="JSON files" data-type="indexterm">&nbsp;</a>JSON files. Each transaction is recorded as a new JSON file in <a contenteditable="false" data-primary="_delta_log directory" data-primary-sortas="delta_log directory" data-type="indexterm">&nbsp;</a>the <em>_delta_log</em> directory, which includes metadata about the transaction, such as the operation performed, the files added or removed, and the schema of the table at the time of the transaction.</p>
	</dd>
	<dt>Metadata</dt>
	<dd>
	<p>Metadata in <a contenteditable="false" data-primary="metadata" data-type="indexterm">&nbsp;</a>Delta Lake includes information about the table’s schema, partitioning, and configuration settings. This metadata is stored in the transaction log and can be retrieved using SQL, Spark, Rust, and Python APIs. The metadata helps manage and optimize the table by providing information for schema enforcement and evolution, partitioning strategies, and data skipping.</p>
	</dd>
	<dt>Schema</dt>
	<dd>
	<p>A Delta Lake table’s schema defines the data’s structure, including its columns, data types, and so on. The schema is enforced on write, ensuring that all data written to the table adheres to the defined structure. Delta Lake supports schema evolution (add new columns, rename columns, etc.), allowing the schema to be updated as the data changes over time.</p>
	</dd>
	<dt>Checkpoints</dt>
	<dd>
	<p>Checkpoints <a contenteditable="false" data-primary="checkpointing" data-type="indexterm">&nbsp;</a>are periodic snapshots of the transaction log that help speed up the recovery process. Delta Lake consolidates the state of the transaction log by default every 10 transactions. This allows client readers to quickly catch up from the most recent checkpoint rather than replaying the entire transaction log from the beginning. Checkpoints are stored as Parquet files and are created automatically by Delta Lake.</p>
	</dd>
</dl>

<p><a data-type="xref" href="#ch01_figure_5_1726062516569774">#ch01_figure_5_1726062516569774</a> is a graphical representation of the structure of a Delta Lake <a contenteditable="false" data-primary="Delta Lake tables (Delta tables)" data-secondary="anatomy of" data-startref="icd106" data-type="indexterm">&nbsp;</a>table.</p>

<figure id="ch01_figure_5_1726062516569774"><img src="images/dldg_0105.png" />
<figcaption>Delta Lake table layout for the transaction log and data files (adapted from an image by Denny Lee)<span data-type="footnote">Denny Lee, <a href="https://oreil.ly/X5p5R">“Understanding the Delta Lake Transaction Log at the File Level”</a>, <em>Denny Lee</em> (blog), November 26, 2023.</span></figcaption>
</figure>
</section>

<section data-type="sect1" id="ch01_delta_transaction_protocol_1726062516601964">
<h1>Delta Transaction Protocol</h1>

<p>In <a contenteditable="false" data-primary="Delta Lake" data-secondary="transaction log protocol" data-type="indexterm" id="icd109">&nbsp;</a>the previous section, we described the anatomy of a Delta Lake table. The <a href="https://oreil.ly/CPxmm">Delta transaction log protocol</a> is the specification defining how clients interact with the table in a consistent manner. At its core, all interactions with the Delta table must begin by reading the Delta <a contenteditable="false" data-primary="transaction log" data-type="indexterm">&nbsp;</a>transaction log to know what files to read. When a client modifies the data, the client initiates the creation of <a contenteditable="false" data-primary="Parquet files" data-type="indexterm">&nbsp;</a>new <a contenteditable="false" data-primary="data files" data-type="indexterm">&nbsp;</a>data files (i.e., Parquet files) and then inserts new <a contenteditable="false" data-primary="metadata" data-type="indexterm">&nbsp;</a>metadata into the transaction log to commit modifications to the table. In fact, many of the original <a href="https://oreil.ly/vfuS2">Delta Lake integrations</a> (delta-spark, Trino connector, delta-rust API, etc.) had codebases maintained by different communities. A Rust client could write, a Spark client could modify, and a Trino client could read from the same Delta table without conflict because they all independently followed the same protocol.</p>

<p>Implementing this specification brings ACID properties to large data collections stored as files in a distributed filesystem or object store. As defined in the specification, the protocol was designed with the following goals in mind:</p>

<dl>
	<dt>Serializable ACID writes</dt>
	<dd>
	<p>Multiple writers can modify a Delta table concurrently while maintaining ACID semantics.</p>
	</dd>
	<dt>Snapshot isolation for reads</dt>
	<dd>
	<p>Readers can read a <a contenteditable="false" data-primary="snapshot isolation for reads" data-type="indexterm">&nbsp;</a>consistent snapshot of a Delta table, even in the face of concurrent writes.</p>
	</dd>
	<dt>Scalability to billions of partitions or files</dt>
	<dd>
	<p>Queries against a Delta table can be planned on a single machine or in parallel.</p>
	</dd>
	<dt>Self-describing</dt>
	<dd>
	<p>All metadata for a Delta table is stored alongside the data. This design eliminates the need to maintain a separate metastore to read the data and allows static tables to be copied or moved using standard filesystem tools.</p>
	</dd>
	<dt>Support for incremental processing</dt>
	<dd>
	<p>Readers can tail the Delta log to determine what data has been added in a given period of time, allowing for efficient streaming.</p>
	</dd>
</dl>

<section data-type="sect2" id="ch01_understanding_the_delta_lake_transaction_log_at_th_1726062516602037">
<h2>Understanding the Delta Lake Transaction Log at the File Level</h2>

<p>To<a contenteditable="false" data-primary="Delta transaction log protocol" data-secondary="file level" data-type="indexterm">&nbsp;</a> better understand this in action, let’s look at what happens at the file level when a Delta table is created. Initially, the table’s transaction log is automatically created in <a contenteditable="false" data-primary="_delta_log directory" data-primary-sortas="delta_log directory" data-type="indexterm">&nbsp;</a>the <em>_delta_log</em> subdirectory. As changes are made to the table, the operations are recorded as <a contenteditable="false" data-primary="atomic commits" data-type="indexterm">&nbsp;</a>ordered <em>atomic commits</em> in the transaction log. Each commit is written out as a JSON file, starting with <em>000...00000.json</em>. Additional changes to the table generate subsequent <a contenteditable="false" data-primary="JSON files" data-type="indexterm">&nbsp;</a>JSON files in ascending numerical order, so that the next commits are written out as <em>000...00001.json</em>, <em>000...00002.json</em>, and so on. Each numeric JSON file increment represents a new version of the table, as described in <a data-type="xref" href="#ch01_figure_5_1726062516569774">#ch01_figure_5_1726062516569774</a>.</p>

<p>Note how the structure of the <a contenteditable="false" data-primary="data files" data-type="indexterm">&nbsp;</a>data files has not changed; they exist as separate <a contenteditable="false" data-primary="Parquet files" data-type="indexterm">&nbsp;</a>Parquet files generated by the query engine or language writing to the Delta table. If your table utilizes Hive-style partitioning, you will retain the same structure.</p>
</section>

<section data-type="sect2" id="ch01_the_single_source_of_truth_1726062516602093">
<h2>The Single Source of Truth</h2>

<p>Delta<a contenteditable="false" data-primary="Delta transaction log protocol" data-secondary="as single source of truth" data-secondary-sortas="single source of truth" data-type="indexterm">&nbsp;</a> Lake <a contenteditable="false" data-primary="single source of truth" data-type="indexterm">&nbsp;</a>allows multiple readers and writers of a given table to all work on the table at the same time. It is the central repository that tracks all user changes to the table. This concept is important because, over time, processing jobs will invariably fail in your data lake. The result is partial files that are not removed. Subsequent processing or queries will not be able to ascertain which files should or should not be included in their queries. To show users correct views of the data at all times, the Delta log is the <em>single source of truth</em>.</p>
</section>

<section data-type="sect2" id="ch01_the_relationship_between_metadata_and_data_1726062516602151" class="pagebreak-before less_space">
<h2>The Relationship Between Metadata and Data</h2>

<p>As<a contenteditable="false" data-primary="Delta transaction log protocol" data-secondary="metadata–data relationship" data-type="indexterm">&nbsp;</a> the <a contenteditable="false" data-primary="metadata" data-secondary="relationships between data and" data-type="indexterm">&nbsp;</a>Delta transaction log is the single source of truth, any client who wants to read or write to your Delta table <em>must</em> first query the transaction log. For example, when inserting data while creating our Delta table, we initially generate two Parquet files: <em>1.parquet</em> and <em>2.parquet</em>. This event would automatically be added to the transaction log and saved to disk as commit <em>000...00000.json</em> (see A in <a data-type="xref" href="#ch01_figure_6_1726062516569796">#ch01_figure_6_1726062516569796</a>).</p>

<figure id="ch01_figure_6_1726062516569796"><img alt="A screenshot of a computer program

Description automatically generated" src="images/dldg_0106.png" />
<figcaption><em>(left)</em> Creating a new Delta table by adding Parquet files and their relationship with the Delta transaction log; <em>(right)</em> deleting rows from this Delta table by removing and adding files and their relationship with the Delta transaction log</figcaption>
</figure>

<p>In a subsequent command (B in <a data-type="xref" href="#ch01_figure_6_1726062516569796">#ch01_figure_6_1726062516569796</a>), we run a <code>DELETE</code> operation that results in the removal of rows from the table. Instead of modifying the existing Parquet files (<em>1.parquet</em>, <em>2.parquet</em>), Delta creates a third file (<em>3.parquet</em>).</p>
</section>

<section data-type="sect2" id="ch01_multiversion_concurrency_control_mvcc_file_and_d_1726062516602210">
<h2>Multiversion Concurrency Control (MVCC) File and Data Observations</h2>

<p>For <a contenteditable="false" data-primary="Delta transaction log protocol" data-secondary="multiversion concurrency control" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="MVCC (multiversion concurrency control)" data-type="indexterm">&nbsp;</a>deletes on object stores, it is faster to create a new file or files comprising the unaffected rows rather than modifying the existing Parquet file(s). This approach also provides the advantage of multiversion concurrency control (MVCC). MVCC is a database optimization technique that creates copies of the data, thus allowing data to be safely read and updated concurrently. This technique also allows Delta Lake to provide <a contenteditable="false" data-primary="time travel feature" data-type="indexterm">&nbsp;</a>time travel. Therefore, Delta Lake creates multiple files for these actions, providing atomicity, MVCC, and speed.</p>

<div data-type="note">
<p>We can speed up this process by using <a contenteditable="false" data-primary="deletion vectors" data-type="indexterm">&nbsp;</a>deletion vectors, an approach we will describe in <a data-type="xref" href="#ch08_advanced_features_1726062528947116">#ch08_advanced_features_1726062528947116</a>.</p>
</div>

<p>The removal/creation of the Parquet files shown in B in <a data-type="xref" href="#ch01_figure_6_1726062516569796">#ch01_figure_6_1726062516569796</a> is wrapped in a single transaction recorded in the Delta transaction log in the file <em>000...00001.json</em>. Some important observations concerning atomicity are:</p>

<ul>
	<li>
	<p>If a user were to read the Parquet files without reading the Delta transaction log, they would read duplicates because of the replicated rows in all the files (<em>1.parquet</em>, <em>2.parquet</em>, <em>3.parquet</em>).</p>
	</li>
	<li>
	<p>The remove and add actions are wrapped in the single transaction log <em>000...00001.json</em>. When a client queries the Delta table at this time, it records both of these actions and the filepaths for that snapshot. For this transaction, the filepath would point only to <em>3.parquet</em>.</p>
	</li>
	<li>
	<p>Note that the remove operation is a soft delete or tombstone where the physical removal of the files (<em>1.parquet</em>, <em>2.parquet</em>) has yet to happen. The physical removal of files will happen when executing <a contenteditable="false" data-primary="vacuum operation" data-type="indexterm">&nbsp;</a>the <code>VACUUM</code> command.</p>
	</li>
	<li>
	<p>The previous transaction <em>000...00000.json</em> has the filepath pointing to the original files (<em>1.parquet</em>, <em>2.parquet</em>). Thus, when querying for an older version of the Delta table via <a contenteditable="false" data-primary="time travel feature" data-type="indexterm">&nbsp;</a>time travel, the transaction log points to the files that make up that older snapshot.</p>
	</li>
</ul>
</section>

<section data-type="sect2" id="ch01_observing_the_interaction_between_the_metadata_and_1726062516602270">
<h2>Observing the Interaction Between the Metadata and Data</h2>

<p>While <a contenteditable="false" data-primary="Delta transaction log protocol" data-secondary="metadata–data interactions" data-type="indexterm" id="icd110">&nbsp;</a>we <a contenteditable="false" data-primary="metadata" data-secondary="metadata–data interactions" data-type="indexterm" id="kab001">&nbsp;</a>now have a better understanding of what happens at the <em>individual</em> data file and metadata file level, how does this all work together? Let’s look at this problem by following the flow of <a data-type="xref" href="#ch01_figure_7_1726062516569826">#ch01_figure_7_1726062516569826</a>, which represents a common data processing failure scenario. The table is initially represented by two <a contenteditable="false" data-primary="data processing failure scenario" data-type="indexterm">&nbsp;</a>Parquet <a contenteditable="false" data-primary="partial files" data-type="indexterm">&nbsp;</a>files (<em>1.parquet</em> and <em>2.parquet</em>) at t<sub>0</sub>.</p>

<figure id="ch01_figure_7_1726062516569826"><img src="images/dldg_0107.png" />
<figcaption>A common data processing failure scenario: partial files</figcaption>
</figure>

<p>At t<sub>1</sub>, job 1 <a contenteditable="false" data-primary="Parquet files" data-secondary="partial files" data-type="indexterm">&nbsp;</a>extracts file 3 and file 4 and writes them to storage. However, due to some error (network hiccup, storage temporarily offline, etc.), an incomplete portion of file 3 and none of file 4 are written into <em>3.parquet</em>. Thus, <em>3.parquet</em> is a partial file, and this incomplete data will be returned to any clients that subsequently query the files that make up this table.</p>

<p>To complicate matters, at t<sub>2</sub>, a new version of the same processing job (job 1 v2) successfully completes its task. It generates a new version of <em>3.parquet</em> and <em>4.parquet</em>. But because the partial <em>3'.parquet</em> (circled) exists alongside <em>3.parquet</em>, any system querying these files will result in <a contenteditable="false" data-primary="double counting" data-type="indexterm">&nbsp;</a>double counting.</p>

<p>However, because the Delta transaction log tracks which files are valid, we can avoid the preceding scenario. Thus, when a client reads a Delta Lake table, the engine (or API) initially verifies the transaction log to see what new transactions have been posted to the table. It then updates the client table with any new changes. This ensures that any client’s version of a table is always synchronized. Clients cannot make divergent, conflicting changes to a table.</p>

<p>Let’s repeat the same partial file example on a Delta Lake table. <a data-type="xref" href="#ch01_figure_8_1726062516569861">#ch01_figure_8_1726062516569861</a> shows the same scenario in which the table is represented by two Parquet files (i.e., <em>1.parquet</em> and <em>2.parquet</em>) at t<sub>0</sub>. The transaction log records that these two files make up the Delta table at t<sub>0</sub> (Version 0).</p>

<figure id="ch01_figure_8_1726062516569861"><img alt="A screenshot of a computer

Description automatically generated" src="images/dldg_0108.png" />
<figcaption>Delta Lake avoids the partial files scenario because of its transaction log</figcaption>
</figure>

<p>At t<sub>1</sub>, job 1 fails with the creation of <em>3.parquet</em>. However, because the job failed, the transaction was <em>not </em>committed to the transaction log. No new files are recorded; notice how the transaction log has only <em>1.parquet</em> and <em>2.parquet</em> listed. Any queries against the Delta table at t<sub>1</sub> will read only these two files, even if other files are in storage.</p>

<p>At t<sub>2</sub>, job 1 v2 is completed, and its output is the files <em>3.parquet</em> and <em>4.parquet</em>. Because the job was successful, the Delta log includes entries only for the two successful files. That is, <em>3'.parquet</em> is <em>not</em> included in the log. Therefore, any clients querying the Delta table at t<sub>2</sub> will see only the <a contenteditable="false" data-primary="metadata" data-secondary="metadata–data interactions" data-startref="kab001" data-type="indexterm">&nbsp;</a>correct <a contenteditable="false" data-primary="Delta transaction log protocol" data-secondary="metadata–data interactions" data-startref="icd110" data-type="indexterm">&nbsp;</a>files.</p>
</section>

<section data-type="sect2" id="ch01_table_features_1726062516602330">
<h2>Table Features</h2>

<p>Originally, <a contenteditable="false" data-primary="Delta transaction log protocol" data-secondary="table features" data-type="indexterm" id="icd110a">&nbsp;</a><a contenteditable="false" data-primary="Delta Lake tables (Delta tables)" data-secondary="features of" data-type="indexterm" id="icd111">&nbsp;</a>Delta tables used <a href="https://oreil.ly/xkjRV">protocol versions</a> to map to a set of features to ensure user workloads did not break when new features in Delta were released. For example, if a client wanted to use <a contenteditable="false" data-primary="CDF (Change Data Feed)" data-type="indexterm">&nbsp;</a>Delta’s <a href="https://oreil.ly/dJeON">Change Data Feed (CDF) option</a>, users were required to upgrade their protocol versions and validate their workloads to access new features (<a data-type="xref" href="#ch01_figure_9_1726062516569880">#ch01_figure_9_1726062516569880</a>). This ensured that any readers or writers incompatible with a specific protocol version were blocked from reading or writing to that table to <a contenteditable="false" data-primary="Delta writer protocol versions" data-type="indexterm">&nbsp;</a>prevent data corruption.</p>

<figure id="ch01_figure_9_1726062516569880"><img src="images/dldg_0109.png" />
<figcaption>Delta writer protocol versions</figcaption>
</figure>

<p>But this process slows feature adoption because it requires the client and table to support <em>all</em> features in that protocol version. For example, with protocol version 4, your Delta table supports both <a href="https://oreil.ly/E7sBs">generated columns</a> and CDF. For your client to read this table, it must support both <a contenteditable="false" data-primary="generated columns" data-type="indexterm">&nbsp;</a>generated <a contenteditable="false" data-primary="columns, generated" data-type="indexterm">&nbsp;</a>columns and Change Data Feed even if you only want to use CDF. In other words, Delta connectors have no choice but to implement all features just to support a single feature in the new version.</p>

<p>Introduced in <a href="https://oreil.ly/Apr1l">Delta Lake 2.3.0</a>, <em>Table Features</em> replaces <a contenteditable="false" data-primary="table protocol versions" data-type="indexterm">&nbsp;</a>table protocol versions to represent features a table uses so connectors can know which features are required to read or write a table (<a data-type="xref" href="#ch01_figure_10_1726062516569898">#ch01_figure_10_1726062516569898</a>).</p>

<figure id="ch01_figure_10_1726062516569898"><img alt="A diagram of a software application

Description automatically generated" src="images/dldg_0110.png" />
<figcaption>Delta Lake Table Features</figcaption>
</figure>

<p>The advantage of this approach is that any connectors (or integrations) can selectively implement certain features of their interest, instead of having to work on all of them. A quick way to view what table features are enabled is to run the <a contenteditable="false" data-primary="SHOW TBLPROPERTIES query" data-type="indexterm">&nbsp;</a>query <code>SHOW TBLPROPERTIES</code>:</p>

<pre data-code-language="sql" data-type="programlisting">SHOW TBLPROPERTIES default.my_table;</pre>

<p>The output would look similar to the following:</p>

<pre data-type="programlisting" class="pagebreak-after">Key (String)                         Value (String)
delta.minReaderVersion               3
delta.minWriterVersion               7
delta.feature.deletionVectors        supported
delta.enableDeletionVectors          true
delta.checkpoint.writeStatsAsStruct  true
delta.checkpoint.writeStatsAsJson    false
</pre>

<p>To dive deeper, please refer<a contenteditable="false" data-primary="Delta Lake" data-secondary="transaction log" data-startref="icd109" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Delta transaction log protocol" data-startref="icd108" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Delta transaction log protocol" data-secondary="table features" data-startref="icd110a" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Delta Lake tables (Delta tables)" data-secondary="features of" data-startref="icd111" data-type="indexterm">&nbsp;</a> to <a href="https://oreil.ly/wjBBu">“Table Features” in the GitHub page for the Delta transaction protocol</a>.</p>
</section>
</section>

<section data-type="sect1" id="ch01_delta_kernel_1726062516602393">
<h1>Delta Kernel</h1>

<p>As <a contenteditable="false" data-primary="Delta Kernel" data-type="indexterm" id="icd113">&nbsp;</a><a contenteditable="false" data-primary="Delta Lake" data-secondary="Delta Kernel" data-type="indexterm" id="icd114">&nbsp;</a>previously noted, Delta Lake provides ACID guarantees and performance across many frameworks, services, and languages. As of this writing, every time new features are added to Delta Lake, the connector must be rewritten entirely, because there is a tight coupling between the metadata and data processing. <a href="https://oreil.ly/0P0zv">Delta Kernel</a> simplifies the development of <a contenteditable="false" data-primary="connectors" data-secondary="simplifying development of with Delta Kernel" data-type="indexterm">&nbsp;</a>connectors by abstracting out all the protocol details so the connectors do not need to understand them. <a contenteditable="false" data-primary="Kernel" data-see="Delta Kernel" data-type="indexterm">&nbsp;</a>Kernel itself implements the Delta transaction log specification (per the previous section). This allows the connectors to build only against the Kernel library, which provides the following advantages:</p>

<dl>
	<dt>Modularity</dt>
	<dd>
	<p>Creating Delta Kernel allows for more easily maintained parity between <a contenteditable="false" data-primary="Delta Lake Rust" data-type="indexterm">&nbsp;</a>Delta Lake Rust and <a contenteditable="false" data-primary="Scala/JVM" data-type="indexterm">&nbsp;</a>Scala/JVM, enabling both to be first-class citizens. All <a contenteditable="false" data-primary="metadata" data-secondary="coordinated and executed through Kernel library" data-type="indexterm">&nbsp;</a>metadata (i.e., transaction log) logic is coordinated and executed through the Kernel library. This way, the connectors need only to focus on how to perform their respective frameworks/services/languages. For example, the <a contenteditable="false" data-primary="Apache Flink" data-secondary="delta-flink connector" data-type="indexterm">&nbsp;</a>Apache Flink/Delta Lake connector needs to focus only on reading or modifying the specific files provided by Delta Kernel. The end client does not need to understand the semantics of the transaction log.</p>
	</dd>
	<dt>Extensibility</dt>
	<dd>
	<p>Delta Kernel decouples the <em>logic</em> for the <a contenteditable="false" data-primary="metadata" data-secondary="decoupling logic around metadata from data" data-type="indexterm">&nbsp;</a>metadata (i.e., transaction log) from the data. This allows Delta Lake to be modular, extensible, and highly portable (for example, you can copy the entire table with its transaction log to a new location for your AI workloads). This also extends (pun intended) to Delta Lake’s extensibility, as a connector is now, for example, provided the list of files to read instead of needing to query the transaction log directly. Delta Lake already has many <a href="https://oreil.ly/nlO4w">integrations</a>, and by decoupling the logic around the metadata from the data, it will be easier for all of us to maintain our various connectors.</p>
	</dd>
</dl>

<p>Delta Kernel achieves this level of abstraction through the following requirements:</p>

<dl>
	<dt>It provides narrow, stable APIs for connectors.</dt>
	<dd>
	<p>For a table <a contenteditable="false" data-primary="APIs (application programming interfaces), for connectors, narrow and stable" data-type="indexterm">&nbsp;</a>scan <a contenteditable="false" data-secondary-sortas="APIs, narrow and stable" data-primary="connectors" data-secondary="for APIs, narrow and stable" data-type="indexterm">&nbsp;</a>query, a connector needs to specify only the query schema, so that the Kernel can read only the required columns, and the query filters for Kernel to skip data (files, rowgroups, etc.). APIs will be stable and backward compatible. Connectors should be able just to upgrade the Delta Kernel version without rewriting their client code—that is, they automatically get support for an updated Delta protocol via Table Features.</p>
	</dd>
	<dt>It internally implements the protocol-specific logic.</dt>
	<dd>
	<p>Delta Kernel will implement all of the following operations:</p>

	<ul>
		<li>
		<p>Read JSON files</p>
		</li>
		<li>
		<p>Read Parquet log files</p>
		</li>
		<li>
		<p>Replay log with data skipping</p>
		</li>
		<li>
		<p>Read Parquet data and DV files</p>
		</li>
		<li>
		<p>Transform data (e.g., filter by DVs)</p>
		</li>
	</ul>

	<p>While Kernel internally implements the protocol-specific logic, better engine-specific implementations can be added (e.g., Apache Spark or Trino may have better JSON and Parquet reading capabilities).</p>
	</dd>
	<dt>It provides APIs for plugging in better performance.</dt>
	<dd>
		<p>These include <em>Table APIs</em> for connectors to perform table operations such as data scans and <em>Engine APIs</em> for plugging in connector-optimized implementations for performance-sensitive components.</p>
	</dd>
</dl>

<p>As of this writing, Delta Kernel is still in the early stages, and building your own Kernel connector is outside the scope of this book. If you would like to dive deeper into how to build your own <a contenteditable="false" data-primary="Delta Kernel" data-secondary="resources about" data-type="indexterm">&nbsp;</a>Kernel connector, please refer to the following<a contenteditable="false" data-primary="Delta Kernel" data-startref="icd113" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Delta Lake" data-secondary="Delta Kernel" data-startref="icd114" data-type="indexterm">&nbsp;</a> resources:</p>

<ul>
	<li>
	<p><a href="https://oreil.ly/a6Qn9">“[Umbrella Feature Request] Delta Kernel APIs to simplify building connectors for reading Delta tables”</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/_KRTc">Delta Kernel—Java</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/Yx4-a">Delta Kernel—Rust</a></p>
	</li>
	<li>
	<p><a href="https://oreil.ly/u3FpF">“Delta Kernel: Simplifying Building Connectors for Delta”</a></p>
	</li>
</ul>
</section>

<section data-type="sect1" id="ch01_delta_uniform_1726062516602450">
<h1>Delta UniForm</h1>

<p>As <a contenteditable="false" data-primary="Delta Lake" data-secondary="Delta UniForm" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Delta UniForm (Universal Format)" data-type="indexterm">&nbsp;</a>noted in the section <a data-type="xref" href="#ch01_lakehouses_or_data_lakehouses_1726062516601476">#ch01_lakehouses_or_data_lakehouses_1726062516601476</a>, there are multiple lakehouse formats. Delta Universal Format, or <a contenteditable="false" data-primary="UniForm" data-see="Delta UniForm" data-type="indexterm">&nbsp;</a>UniForm, is designed to simplify the interoperability among Delta Lake, <a contenteditable="false" data-primary="Apache Iceberg" data-type="indexterm">&nbsp;</a>Apache <a contenteditable="false" data-primary="Iceberg, Apache" data-type="indexterm">&nbsp;</a>Iceberg, and <a contenteditable="false" data-primary="Apache Hudi" data-type="indexterm">&nbsp;</a>Apache <a contenteditable="false" data-primary="Hudi, Apache" data-type="indexterm">&nbsp;</a>Hudi. Fundamentally, lakehouse formats are composed of metadata and data (typically in Parquet file format).</p>

<p>What makes these lakehouse formats different is how they create, manage, and maintain the <em>metadata</em> associated with this data. With Delta UniForm, the <a contenteditable="false" data-primary="metadata" data-secondary="concurrent generation of lakehouse formats metadata with Delta format" data-type="indexterm">&nbsp;</a>metadata of other lakehouse formats is generated concurrently with the Delta format. This way, whether you have a Delta, Iceberg, or Hudi client, it can read the data, because all of their APIs can understand the metadata. Delta UniForm includes the following support:</p>

<ul>
	<li>
	<p>Apache Iceberg support as part of <a href="https://oreil.ly/BheSC">Delta Lake 3.0.0</a> (October 2023)</p>
	</li>
	<li>
	<p>Apache Hudi support as part of <a href="https://oreil.ly/MgmwB">Delta Lake 3.2.0</a> (May 2024)</p>
	</li>
</ul>

<p>For the latest information on how to enable these features, please refer to the <a href="https://oreil.ly/1hQoF">Delta UniForm documentation</a>.</p>
</section>

<section data-type="sect1" id="ch01_conclusion_1726062516602501">
<h1>Conclusion</h1>

<p>In this chapter, we explained the origins of Delta Lake, what it is and what it does, its anatomy, and the transaction protocol. We emphasized that the Delta transaction log is the single source of truth and thus is the single source of the relationship between its metadata and data. While still early, this has led to the development of Delta Kernel as the foundation for simplifying the building of Delta connectors for Delta Lake’s many frameworks, services, and community projects. The core difference between the different lakehouse formats is their metadata, so Delta UniForm unifies them by generating all formats’ <a contenteditable="false" data-primary="Delta Lake" data-startref="icd101" data-type="indexterm">&nbsp;</a>metadata.</p>
</section>
</section>
