[[ch01-introduction]]
== Introduction to CockroachDB

CockroachDB is a distributed, transactional, relational, cloud native SQL database system. That's quite a mouthful!  But in short, CockroachDB leverages both the strengths of the previous generation of relational database systems—strong consistency, the power of SQL, and the relational data model—and the strengths of modern distributed cloud principles.  The result is a database system that is broadly compatible with other SQL-based transactional databases but delivers much greater scalability and availability.

In this chapter, we'll review the history of database management systems (DBMSs) and discover how CockroachDB exploits technology advances of the last few decades to deliver on its ambitious goals.
 
=== A Brief History of Databases

Data storage and data processing ((("DBMS (database management system)")))((("database management system (DBMS)")))((("databases", "history", id="dbshsty")))are the "killer apps" of human civilization. Verbal language gave us an enormous advantage in cooperating as a community. Still, it was only when we developed data storage—e.g., written language—that each generation could build on the lessons of preceding generations.

The earliest written records—dating back almost 10,000 years—are agricultural accounting records. These ((("cuneiform tablets")))cuneiform records, recorded on clay tablets (<<Figure01-1>>), serve the same purpose as the databases that support modern accounting systems.

Information storage technologies over thousands of years progressed slowly.  The use of cheap, portable, and reasonably durable paper media organized in libraries and cabinets represented best practices for almost a millennia.

[[Figure01-1]] 
.Cuneiform tablet circa 3000 BC (Source: https://cockroa.ch/3AFZ9eY[Wikipedia])
image::images/cdb2_0101.png[Cuniform table circa 3000 BC]

The emergence of digital data processing has truly resulted in an information revolution. Within a single human life span, digital information systems have resulted in exponential growth in the volume and rate of information storage.  Today, the vast bulk of human information is stored in digital formats, much of it within database systems.

==== Pre-Relational Databases

The first digital computers had ((("relational databases", "pre-relational databases", id="rldtprdb")))((("pre-relational databases", id="preredtb")))((("databases", "relational", see="relational databases")))negligible storage capacity and were used primarily for computation—for instance, generating ballistic tables, decrypting codes, and performing scientific calculations. However, as magnetic tape and disks became mainstream in the 1950s, it became increasingly possible to use computers to store and process volumes of information that would be unwieldy by other means.

Early applications used ((("flat files")))((("databases", "flat files")))simple flat files for data storage. But it soon became obvious that the complexities of reliably and efficiently dealing with large amounts of data required specialized and dedicated software platforms—and these became the first data systems.

Early database systems ran within monolithic mainframe computers, which also were responsible for the application code. The applications were tightly coupled with the database systems and processed data directly using procedural language directives. By the 1970s, two models ((("databases", "network models")))((("databases", "hierarchical models")))((("databases", "IMS (Information Management System)")))((("Information Management System (IMS)")))((("IMS (Information Management System)")))((("IDMS (Integrated Database Management System)")))((("Integrated Database Management System (IDMS)")))of database systems were vying for dominance: the _network_ and _hierarchical_ models. These models were represented by the major databases of the day, _IMS_ (Information Management System) and _IDMS_ (Integrated Database Management System).

These systems were great advances on their predecessors but had significant drawbacks. Queries needed to be anticipated in advance of implementation, and only record-at-a-time processing ((("relational databases", "pre-relational databases", startref="rldtprdb")))((("pre-relational databases", startref="preredtb")))was supported. Even the simplest report required programming resources to implement, and all IT departments suffered from a huge backlog of reporting requests.

==== The Relational Model

Probably no one has had more influence ((("relational databases", "Codd, Edgar", id="rldcdd")))((("Codd, Edgar", id="cdddg")))over database technology than Edgar Codd (whatever Larry Ellison might think).  Codd was a “programming mathematician”—what we might today call a data scientist—who had worked at IBM on and off since 1949.  In  1970, Codd wrote his seminal paper, https://cockroa.ch/3LICx1M["A Relational Model of Data for Large Shared Data Banks"]. This paper outlined what Codd saw as fundamental issues in the design of existing database systems:

* Existing database systems merged physical and logical representations of data in a way that often complicated requests for data and created difficulties in satisfying requests that were not anticipated during database design.

* There was no formal standard for data representation&mdash;as a mathematician, Codd was familiar with theoretical models for representing data—and he believed these principles should be applied to database systems.

* Existing database systems were too hard to use.  Only programmers were able to retrieve data from these systems, and the process of retrieving data was needlessly complex.  Codd felt that there needed to be an easy-access method for data retrieval.

Codd’s relational model described ((("query languages")))a means of logically representing data that was independent of the underlying storage mechanism. It required a _query language_ that could be used to answer any question that could be satisfied by the data.

The relational model defines the fundamental building blocks of a relational database:

* _Tuples_ are a set of _attribute_ values.  Attributes ((("tuples")))((("attributes, tuples")))are named scalar (single-dimensional) values.  A tuple can be thought of as an individual "record" or "row."

* A _relation_ is a collection of distinct tuples of the same form.  A relation represents a two-dimensional data set with a fixed number of attributes and an arbitrary number of tuples&mdash;a table in a database is an example of a relation.

* _Constraints_ enforce consistency and define ((("constraints")))relationships between tuples.

* Various _operations_ are defined, such as joins, projections, and unions. Operations on relations always return relations. For instance, when you join two relations, the result is itself a relation.

* A _key_ consists ((("keys")))of one or more attributes that can be used to identify a tuple.  There can be more than one key, and a key can consist of multiple attributes.

The relational model furthermore ((("third normal form (TNF)")))((("TNF", see="third normal form")))defined a series of "normal forms" that represent reducing levels of redundancy in the model.  A relation is in _third normal form_ if all the data in each tuple is dependent on the entire primary key of that tuple and on no other attributes.  We generally remember this by the adage, "The key, the whole key, and nothing but the key (so help me, Codd)."

Third normal form generally represents the starting ((("relational databases", "Codd, Edgar", startref="rldcdd")))((("Codd, Edgar", startref="cdddg")))point for the construction of an efficient and performant data model. We will come back to third normal form in <<Ch05_CockroachDB_Schema_Design>>.  <<Figure1-2>> illustrates data in third normal form.

 
[[Figure1-2]] 
image::images/cdb2_0102.png[]

[role="pagebreak-before less_space"]
==== Implementing the Relational Model

The relational model served as the ((("tuples")))((("tables")))foundation for the familiar structures present in all relational databases today.  Tuples are represented as _rows_ and relations as _tables_.

A table is a relation that has been given physical storage. The underlying storage may take different forms.   In addition to the physical representation of the data, indexing and clustering schemes were introduced to facilitate efficient data processing and implement constraints. 

Indexes and clustered storage were not a part of the relational model, but they were incorporated in relational databases to transparently enhance query performance without changing the types of queries that could be performed.  Thus, the logical representation of the data as presented to the application was independent of the underlying physical model. 

Indeed, in some relational implementations, a table might be implemented by multiple indexed structures, allowing different access paths to the data.

==== Transactions


A transaction is a logical unit ((("relational databases", "transactions")))((("transactions")))of work that must succeed or fail as a unit.  Transactions predated the relational model, but in pre-relational systems, transactions were often the responsibility of the application layer.  In Codd's relational model, the database took formal responsibility for transactional processing.footnote:[From “Rule 5” in Codd’s 12 rules, which were published in the early 1980s.] In Codd's formulation, a relational system would provide explicit support for commencing a transaction and either committing or aborting that transaction.

The use of transactions to maintain consistency in application data was also used internally to maintain consistency between the various physical structures that represented tables.  For instance, when a table is represented in multiple indexes, all of those indexes must be kept synchronized in a transactional manner.

Codd's relational model did not define all the aspects of transactional [.keep-together]#behavior that# became common to most relational database systems. In 1981, Jim Gray articulated the core ((("ACID (atomic, consistent, isolated, durable)")))((("atomic, consistent, isolated, durable (ACID)")))principles of transaction processing that we still use today. These principles later became known as __ACID__—atomic, consistent, isolated, and durable—[.keep-together]#transactions#.

As Gray put it, “A transaction is ((("relational databases", "transactions", "ACID")))((("transactions", "ACID")))((("atomicity")))((("durability")))((("consistency")))((("isolation")))a transformation of state which has the properties of _atomicity_ (all or nothing), _durability_ (effects survive failures) and _consistency_ (a correct transformation)." The principle of __isolation__—added in a later revision—required that one transaction should not be able to see the effects of other in-progress [.keep-together]#transactions#.

Perfect isolation ((("isolation", "serializable isolation")))((("serializable isolation")))between transactions—_serializable_ isolation—creates some restrictions on concurrent data processing.  Many databases adopted lower levels of isolation or allowed applications to choose from various isolation levels.  These implications will be discussed further in <<ch02-Architecture>>.

==== The SQL Language

Codd specified ((("relational databases", "transactions", "SQL")))((("transactions", "SQL")))((("SQL (Structured Query Language)")))that a relational system should support a "database sublanguage" to navigate and modify relational data.   He proposed ((("Alpha language")))((("QUEL language")))((("Ingres")))((("PostgreSQL")))((("SQL (Structured Query Language)", "PostgreSQL")))the Alpha language in 1971, which influenced the QUEL language designed by the creators of Ingres—an early relational database system developed at the University of California, which influenced the open source PostgreSQL database.

Meanwhile, researchers at IBM were ((("System R")))developing System R, a prototype DBMS based on Codd's relational model.   They developed the SEQUEL language as the data ((("SEQUEL")))sublanguage for the project.  SEQUEL eventually was renamed SQL and was adopted in commercial IBM databases, including IBM DB2.  

Oracle chose SQL as the query language for its pioneering Oracle relational database management system (RDBMS), and by the end of the 1970s, SQL had won out over QUEL as the relational query language and became an ANSI (American National Standards Institute) standard language in 1986.

SQL needs little introduction.  Today, it's one of the most widely used computer languages in the world. We will devote <<Ch04_CockroachDB_SQL>> to the CockroachDB SQL implementation.    However, it’s worth noting that the relative ease of use that SQL provided expanded the audience of database users dramatically.  No longer did you need to be a highly experienced database programmer to retrieve data from a database&mdash;SQL could be taught to casual users of databases, such as analysts and statisticians.  It’s fair to say that SQL brought databases within reach of business users.

==== The RDBMS Hegemony

The combination of the relational model, SQL language, and ACID transactions became the dominant model for new database systems from the early 1980s through the early 2000s. These systems became ((("RDBMS (relational database management system)")))((("relational database management system (RDBMS)")))((("relational databases", "RDBMS (relational database management system)")))known generically as RDBMS.

The RDBMS came into prevalence around the same time as a seismic paradigm shift in application architectures.  The world of mainframe applications was giving way to ((("client/server model, RDBMS and")))the _client/server_ model.  In the client/server model, application code ran on microcomputers (PCs) while the database ran on a minicomputer, increasingly running the Unix OS.  During the migration to client/server, mainframe-based pre-relational databases were largely abandoned in favor of the new breed of RDBMSs.

By the end of the 20th century, the RDBMS reigned supreme.  The leading commercial databases of the day—Oracle, Sybase, SQL Server, Informix, and DB2—competed on performance, functionality, or price, but all were virtually identical in their adoption of the relational model, SQL, and ACID transactions.  As open source software grew in popularity, open source RDBMSs such as MySQL and PostgreSQL gained significant and growing traction.

==== Enter the Internet

Around the turn of the 21st century, an ((("internet and early databases")))even more important shift in application architectures occurred.  That shift was, of course, the internet.  Initially, internet applications ran on a software stack not dissimilar to a client/server application.  A single large server hosted the application's database, while application code ran on a "middle tier" server and end users interacted with the application through web browsers.

In the early days of the internet, this architecture sufficed—though often just barely.  The monolithic database servers were often a performance bottleneck, and although standby databases were routinely deployed, a database failure was one of the most common causes of application failure.

As the web grew, the limitations of the centralized RDBMS became untenable.  The emerging "web 2.0" social network and ecommerce sites had two characteristics that were ((("monolithic database systems")))((("databases", "monolithic")))increasingly difficult to support:

* These systems had a global or near-global scale.  Users in multiple continents needed simultaneous access to the application.

* Any level of downtime was undesirable.  The old model of "weekend upgrades" was no longer acceptable. There was no maintenance window that did not involve significant business disruption.

All parties agreed that the monolithic single database system would have to give way if the demands of the new breed of internet applications were to be met. It became recognized that a very significant and potentially immovable obstacle stood in the way:  _CAP theorem_.
CAP—or https://cockroa.ch/3KbklgD[Brewer's]—theorem states that you can have at most two of three desirable characteristics in a distributed system (illustrated in <<Figure01-3>>):

Consistency::  Every user sees the same view of the database state.

Availability:: The database remains available unless all elements of the distributed system fail.

Partition tolerance:: The system runs in an environment in which a network partition might divide the distributed system in two, or if two nodes in the network cannot [.keep-together]#communicate,# a partition-tolerant system will continue to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.

 
 
[[Figure01-3]]
.CAP theorem states that a system cannot support all three properties of consistency, availability, and partition tolerance
image::images/cdb2_0103.png["CAP Theorem"]

For instance, consider the case of a global ecommerce system with users in North America and Europe.  If the network between the two continents fails (a network partition), then you must choose one of the following outcomes:

* Users in Europe and North America may see different versions of the database: _sacrificing consistency_.

* One of the two regions needs to shut down (or go read-only): _sacrificing_ [.keep-together]#_availability_#.

Clustered RDBMSs at that time would generally sacrifice availability.  For instance, in Oracle's Real Application Clusters (RAC) clustered database, a network partition between nodes would cause all nodes in one of the partitions to shut down.

Internet pioneers such as Amazon, however, believed ((("Dynamo")))that availability was more important than strict consistency.    Amazon developed a database system—__Dynamo__—that implemented “_eventual consistency_.”  In the event of a partition, all zones would continue to have access to the system, but when the partition was resolved, inconsistencies would be reconciled—possibly losing data in the process.

==== The NoSQL Movement

Between 2008 and 2010, dozens of ((("NoSQL")))new database systems emerged, all of which abandoned the three pillars of the RDBMS: the relational data model, SQL language, and ACID transactions.   Some of these new systems—Cassandra, Riak, Project Voldemort, and HBase, for example—were directly influenced by nonrelational technologies developed at Amazon and Google.

Many of these systems were essentially "schema-free"—supporting or even requiring no specific structure for the data they stored.  In particular, in key-value (KV) databases, an arbitrary key provides programmatic access to an arbitrary structured "value."  The database knows nothing about what is in this value.  From the database's view, the value is just a set of unstructured bits.  Other nonrelational systems represented data in semi-tabular formats or as JSON (JavaScript Object Notation) documents.  However, none of these new databases implemented the principles of the relational model.

These systems were initially referred to as distributed nonrelational database management systems (DNRDBMSs), but—because they did not include the SQL language—rapidly became known by the far catchier term "NoSQL" databases.

NoSQL was always a questionable term.  It defined what the class of systems discarded, rather than their unique distinguishing features. Nevertheless, "NoSQL" stuck, and in the following decade, NoSQL databases such as Cassandra, DynamoDB, and MongoDB became established as a distinct and important segment of the database landscape.

==== The Emergence of Distributed SQL

The challenges of implementing distributed ((("SQL (Structured Query Language)", "distributed", id="sqldstrb")))transactions at a web scale, more than anything else, led to the schism in modern DBMSs.  With the rise of global applications with extremely high uptime requirements, it became unthinkable to sacrifice availability for perfect consistency.  Almost in unison, the leading web 2.0 companies such as Amazon, Google, and Facebook introduced new database services that were only "eventually" or “weakly” consistent but globally and highly available, and the open source community responded with databases based on these principles.

However, NoSQL databases had their own severe limitations.  The SQL language was widely understood and was the basis for almost all business intelligence tools.  NoSQL databases found that they had to offer some SQL compatibility, so many added some SQL-like dialect—leading to the redefinition of NoSQL as "not only SQL." In many cases, these SQL implementations were query-only and intended only to support business intelligence features. In other cases, a SQL-like language supported transactional processing but provided only the most limited subset of SQL functionality.

The problems caused by weakened consistency were harder to ignore.  Consistency and correctness in data are often nonnegotiable for mission-critical applications.  While in some circumstances—social media, for instance—it might be acceptable for different users to see slightly different views of the same topic, in other contexts—such as logistics—any inconsistency is unacceptable.  Advanced nonrelational databases adopted tunable consistency and sophisticated conflict resolution algorithms to mitigate data inconsistency.  However, any database that abandons strict consistency must accept scenarios in which data can be lost or corrupted during the reconciliation of network partitions or from ambiguously timed competing [.keep-together]#transactions#.

Google pioneered many of the technologies behind important open source NoSQL systems.  For instance,  the Google File System and MapReduce technologies led directly to Apache Hadoop, and Google Bigtable led to Apache HBase.  As such, Google was well aware of the limitations of these new data stores.

The Spanner project was initiated as ((("Spanner")))an attempt to build a distributed database, similar to Google's existing Bigtable system, that could support both strong consistency and high availability.

Spanner benefited from Google's highly redundant network, which reduced the probability of network-based availability issues, but the really novel feature of Spanner was its _TrueTime_ system.  TrueTime ((("TrueTime (Spanner)")))explicitly models the uncertainty of time measurement in a distributed system so that it can be incorporated into the transaction protocol. Distributed databases go to a lot of effort to return consistent information from replicas maintained across the system.  Locks are the primary mechanism to prevent inconsistent information from being created in the database, while snapshots are the primary mechanism for returning consistent information.  Queries don't see changes to data that occur while they are executing because they read from a consistent "snapshot" of data.  Maintaining snapshots in distributed databases can be tricky: usually, there's a large amount of inter-node communication required to create agreement on the ordering of transactions and queries. Clock information provided by TrueTime enables the use of snapshots with minimal communication between nodes.

Google Spanner further optimizes the snapshot mechanism by using GPS receivers and atomic clocks installed in each data center.   GPS provides an externally validated timestamp while the atomic clock provides high-resolution time between GPS "fixes." The result is that every Spanner server across the world has almost the same clock time.   This allows Spanner to order transactions and queries precisely without requiring excessive inter-node communication or delays due to excessive clock uncertainty.

[NOTE]
====
Spanner is highly dependent on Google's redundant network and specialized server hardware.  Spanner can't operate independently of the Google network. 
====


The initial version of Spanner pushed the boundaries of the CAP theorem as far as technology allowed.  It represented a distributed database system in which consistency was guaranteed, availability maximized, and network partitions avoided as much as possible. Over time, Google added relational features to the data model of Spanner as well as SQL language support.  By 2017, Spanner had evolved to a distributed database that supported ((("databases", "history", startref="dbshsty")))((("SQL (Structured Query Language)", "distributed", startref="sqldstrb")))all three pillars of the RDBMS: the SQL language, relational data model, and ACID transactions.

=== The Advent of CockroachDB


With Spanner, Google persuasively demonstrated the utility of a highly consistent distributed database.  However, Spanner was tightly coupled to the Google Cloud Platform (GCP) and—at least initially—not publicly available.

There was an obvious need for the technologies pioneered by ((("Kimball, Spencer")))((("Mattis, Peter")))((("Darnell, Ben")))Spanner to be made more widely available.  In 2015, a trio of Google alumni—Spencer Kimball, Peter Mattis, and Ben Darnell—founded Cockroach Labs with the intention of creating an open source, geo-scalable, ACID-compliant database.

Spencer, Peter, and Ben chose the name "CockroachDB" in ((("CockroachDB", "naming")))honor of the humble cockroach, which, it is told, is so resilient that it would survive even a nuclear war (<<Figure01-4>>).

[role="width-30"]
[[Figure01-4]] 
.The original CockroachDB logo
image::images/cdb2_0104.png["The original CockroachDB logo", 200, 100]

==== CockroachDB Design Goals
CockroachDB was designed to support the following attributes:

Scalability::  The CockroachDB distributed ((("CockroachDB", "scalability")))((("scalability")))architecture allows a cluster to scale seamlessly as workload increases or decreases.  Nodes can be added to a cluster without any manual rebalancing, and performance will scale predictably as the number of nodes increases.

High availability:: A CockroachDB cluster ((("CockroachDB", "availability")))((("availability")))has no single point of failure.  CockroachDB can continue operating if a node, zone, or region fails without compromising availability.

Consistency:: CockroachDB provides ((("CockroachDB", "consistency")))((("consistency")))the highest practical level of transactional isolation and consistency.    Transactions operate independently of each other and,  once committed, transactions are guaranteed to be durable and visible to all sessions.

Performance:: The CockroachDB architecture is ((("CockroachDB", "performance")))((("performance")))designed to support low-latency and high-throughput transactional workloads.  Every effort has been made to adopt database best practices with regard to indexing, caching, and other database optimization strategies.

Geo-partitioning::  CockroachDB allows ((("CockroachDB", "geo-partitioning")))((("geo-partitioning")))((("partitioning", "geo-partitioning")))data to be physically located in specific localities to enhance performance for "localized" applications and to respect data sovereignty [.keep-together]#requirements#.

SQL compatibility:: CockroachDB implements ANSI-standard SQL and is wire-protocol compatible with PostgreSQL.  This ((("CockroachDB", "SQL compatibility")))((("SQL (Structured Query Language)", "compatibility")))means that the majority of database drivers and frameworks that work with PostgreSQL will also work with CockroachDB.  Many PostgreSQL applications can be ported to CockroachDB without requiring significant coding changes.

Portability:: CockroachDB is offered as a fully managed database service, which in many cases is the easiest and ((("CockroachDB", "portability")))((("portability")))most cost-effective deployment mode.  But it's also capable of running on pretty much any platform you can imagine, from a developer's laptop to a massive cloud deployment.  The CockroachDB architecture is well aligned with containerized deployment options, and in particular, with Kubernetes.  CockroachDB provides a Kubernetes operator that eliminates much of the complexity involved in a Kubernetes deployment.

You may be thinking, "This thing can do everything!"  However, it's worth pointing out that CockroachDB was not intended to be all things to all people.  In particular:

CockroachDB prioritizes consistency over availability.::  We saw earlier how the CAP theorem states that you have to choose either consistency or availability when faced with a network partition.  Unlike "eventually" consistent databases such as DynamoDB or Cassandra, CockroachDB guarantees consistency at all costs.  This means that there are circumstances in which a CockroachDB node will refuse to service requests if it is cut off from its peers.  A Cassandra node in similar circumstances might accept a request even if there is [.keep-together]#a chance that# the data in the request will later have to be discarded.

The CockroachDB architecture prioritizes transactional workloads.::  CockroachDB includes the SQL constructs ((("CockroachDB", "transactional workloads")))((("transactional workloads")))((("workloads, transactional")))for issuing aggregations and the SQL 2003 analytic "windowing" functions, and CockroachDB is certainly capable of integrating with popular business intelligence tools such as Tableau.  There’s no specific reason why CockroachDB could not be used for analytic applications.  However, the unique features of CockroachDB are targeted more at transactional workloads.  For analytic-only workloads that do not require transactions, other database platforms might provide better performance.

It's important to remember that while CockroachDB was inspired by Spanner, it is in no way a "Spanner clone." The CockroachDB team has leveraged many of the Spanner team's concepts ((("Spanner")))but has diverged from Spanner in several important ways.

First, Spanner was designed to run on very specific hardware.  Spanner nodes have access to an atomic clock and GPS device, allowing incredibly accurate timestamps.  CockroachDB is designed to run well on commodity hardware and within containerized environments (such as Kubernetes) and therefore cannot rely on atomic clock synchronization.  As we will see in <<ch02-Architecture>>, CockroachDB does rely on decent clock synchronization between nodes but is far more tolerant of clock skew than Spanner; as a result, CockroachDB can run anywhere, including any cloud provider or on-premises data center (and one CockroachDB cluster can even span multiple cloud environments).


Second, while the distributed storage engine of CockroachDB is inspired by Spanner, the SQL engine and APIs are designed to be PostgreSQL compatible. PostgreSQL is one of the most implemented RDBMSs today and is supported by an extensive ecosystem of drivers and frameworks.  The "wire protocol" of CockroachDB is completely compatible with PostgreSQL, which means that any driver that works with PostgreSQL will work with CockroachDB.   At the SQL language layer, there will always be differences between PostgreSQL and CockroachDB because of differences in the underlying storage and transaction models. However, most commonly used SQL syntax is shared between the two databases.

Third, CockroachDB has evolved to satisfy the needs of its community and has introduced many features never envisaged by the Spanner project.  Today, CockroachDB is a thriving database platform whose connection to Spanner is only of historical interest.

==== CockroachDB Releases

The first production release of CockroachDB appeared in May 2017. This release introduced the core capabilities ((("CockroachDB", "release history", id="cckrhs")))of the distributed transactional SQL databases, albeit with some limitations of performance and scale. Version 2.0—released in 2018—included new partitioning features for geographically distributed deployments, support for JSON data, and massive improvements in performance.

In 2019, CockroachDB courageously leaped from version 2 to version 19! This was not because of 17 failed versions between 2 and 19 but instead reflected a change in numbering strategy to associate each release with its release year rather than designating releases as "major" or "minor."

Some highlights of past releases include:

* Version 19.1 (April 2019) introduced security features such as encryption at rest and LDAP (Lightweight Directory Access Protocol) integration, the change data capture facility described in <<Ch07_Application_Migration_and_Integration>>, and multiregion optimizations.
* Version 19.2 (November 2019) introduced the Parallel Commits transaction protocol and other performance improvements.
* Version 20.1 (May 2020) introduced many SQL features, including pass:[<code>ALTER <span class="keep-together">PRIMARY KEY</span></code>], `SELECT FOR UPDATE`, nested transactions, and temporary tables.
* Version 20.2 (November 2020) added support for spatial data types, new transaction detail pages in the DB console, and made the distributed `BACKUP` and `RESTORE` functionality available for free.
* Version 21.1 (May 2021) simplified the use of multiregion functionality and expanded logging configuration options.
* Version 21.2 (November 2021) introduced bounded staleness reads and numerous stability and performance improvements, including an admission control system to prevent overloading the cluster.
* Version 22.1 (May 2022) added support for super regions, row-level time-to-live (TTL), and index recommendations.
* Version 22.2 (December 2022) introduced user-defined functions, the MOLT (Migrate Off Legacy Technology) Schema Conversion Tool, trigram indexes, and the insights page.
* Version 23.1 (May 2023) added support for full-text search, user-defined composite types, redaction of PII (Personally Identifiable Information) during EXPLAIN queries, and a new shell editor with tab completion.
* Version 23.2 (February 2024) introduced the READ COMMITTED isolation level, support for PL/pgSQL, and column-level encryption&mdash;additionally, it includes the capability to visualize network partitions in the DB Console, migrate to CockroachDB via Oracle GoldenGate and Debezium, utilize the MOLT Live Migration Service, and perform physical cluster replication.
* Version 24.1 (May 2024) hit a 99.999% (five nines) availability target for multiregion managed service clusters. It also introduced the capability to emit changefeed events to Azure Event Hub, connect managed service clusters to Google Private Service Connect, and included MOLT Fetch.
* Version 24.2 (August 2024) introduced ((("CockroachDB", "release history", startref="cckrhs")))support for the `VECTOR` data type and performance improvements to the cost-based optimizer for prepared statements via generic query plans.

=== CockroachDB in Action

CockroachDB has gained strong and growing traction in a crowded database market.  Users who have been constrained by the scalability of traditional relational databases such as PostgreSQL and MySQL are attracted by the greater scalability of CockroachDB.  Those who have been using distributed NoSQL solutions such as Cassandra are attracted by the greater transactional consistency and SQL compatibility offered by CockroachDB.  And those who are transforming toward modern containerized and cloud native architectures appreciate the cloud and container readiness of the platform.

Today, CockroachDB can boast of significant adoption at scale across multiple industries.  Let’s look at a few of these case studies.footnote:[Cockroach Labs maintains a growing list of https://cockroa.ch/3u6vHwZ[CockroachDB case studies].]

==== CockroachDB at Netflix

Netflix has been using CockroachDB in ((("Netflix")))production since 2020, and in that time has deployed more than 380 large-scale CockroachDB clusters (of which, more than 160 are production clusters and more than 60 of _those_ are multiregion clusters).

Their adoption of CockroachDB is advanced, offering internal teams access to CockroachDB-as-a-service. This allows them to create clusters on-demand, enhance their development speed and agility, and standardize on best practices.

==== CockroachDB at Devsisters

Devsisters is a South Korea–based game development company responsible for games such as the mobile ((("Devsisters")))phone game  _Cookie Run: Kingdom_.   Originally, Devsisters used Couchbase for its persistence layer but was challenged by issues relating to transactional integrity and scalability.  When looking for a new database solution, Devsisters' requirements included scalability, transactional consistency, and support for very high throughput.

Devsisters considered Amazon Aurora and DynamoDB as well as CockroachDB, but in the end, chose CockroachDB.  Sungyoon Jeong from the DevOps team says, “It would have been impossible to scale this game on MySQL or Aurora. We experienced more than six times the workload size we anticipated, and CockroachDB was able to scale with us throughout this journey.”

==== CockroachDB at DoorDash

DoorDash is a local commerce platform ((("DoorDash")))that connects consumers with their favorite businesses across the United States, Canada, Australia, Japan, and Germany. Today, DoorDash has created more than 350 CockroachDB clusters for its developers for various customer-facing, backend analytics, and internal workloads.

The DoorDash team likes that CockroachDB scales horizontally, speaks SQL and has Postgres wire compatibility, and handles heavy reads/writes without impacting performance. CockroachDB's resilient architecture and live schema changes are also a huge bonus for the team. "DoorDash has been able to use CockroachDB to forklift-migrate and scale numerous workloads without having to rewrite applications—only small index or schema changes," says Sean Chittenden, engineering lead for the Core Infrastructure team at DoorDash.

==== CockroachDB at Bose

Bose is a world-leading consumer ((("Bose")))technology company particularly well known as a provider of high-fidelity audio equipment.

Bose’s customer base spans the globe, and Bose aims to provide those customers with best-in-class cloud-based support solutions.   

Bose has embraced modern microservices-based software architecture.  The backbone of the Bose platform is Kubernetes, which allows applications access to low-level services—containerized computation—and to higher-level services such as Elasticsearch, Kafka, and Redis. CockroachDB became the foundation of the database platform for this containerized microservice platform.

[role="pagebreak-before"]
Aside from the resiliency and scalability of CockroachDB, CockroachDB’s capability to be hosted within a Kubernetes environment was decisive.  
 
By running CockroachDB in a Kubernetes environment, Bose has empowered developers by providing a self-service database-on-demand capability.  Developers can spin up CockroachDB clusters for development or testing simply and quickly within a Kubernetes environment.  In production, CockroachDB running with Kubernetes provides full-stack scalability, redundancy, and high availability.

==== CockroachDB at Form3

Form3 is revolutionizing how payments ((("Form3")))work. Powering some of the largest financial institutions in the world, CockroachDB provides a resilient backbone to a tier-0, critical national payments infrastructure.

Form3 regularly tests CockroachDB to its limits with chaos engineering, allowing its built-in self-healing to recover from anything they throw at it. Running CockroachDB across not only multiple cloud regions but also multiple cloud providers makes Form3 one of the most robust and resilient payment providers in the world.

==== CockroachDB at Hard Rock Digital

Answering to the US Federal Wire Act (1961), Hard ((("Hard Rock Digital")))Rock Digital runs a CockroachDB cluster that spans cloud regions and on-prem data centers.

The Federal Wire Act requires that all data and processing for a bet remains within the state it was placed. To reach customers where hyperscalers don't have regions, Hard Rock Digital runs CockroachDB ((("AWS Outpost")))on AWS Outposts&mdash;an AWS service that runs on their own hardware. This allows Hard Rock Digital to run CockroachDB in every state they serve, while remaining compliant with regulations.

==== CockroachDB at Spreedly

Spreedly is a global payments ((("Spreedly")))orchestration platform that gives customers access to payment services in more than 100 countries, all from a single API. Following their adoption of CockroachDB, they've drastically simplified their architecture by:

* Removing multiple legacy databases
* Eliminating unnecessary extract, transform, and load (ETL) pipeline components
* Solving a complex split-brain scenario and ensuring a consistent view of data to all consumers

==== CockroachDB at Route

Route is a leading post-purchase ((("Route")))platform, and CockroachDB is the beating heart of their platform's always-on data model&mdash;processing billions of orders for millions of customers.

Given the seasonal nature of the ecommerce businesses they support, the ease of scaling both up and down afforded by CockroachDB allows them to serve their customers efficiently&mdash;all with zero downtime.

[quote,Brian Call,senior principal engineer at Route]
CockroachDB offers the biggest ROI I’ve seen in terms of paid support from any vendor.

=== Summary

In this chapter, we’ve placed CockroachDB in a historical context and introduced the goals and capabilities of the CockroachDB database.

The RDBMSs that emerged in the 1970s and 1980s were a triumph of software engineering that powered software applications from client/server through to the early internet.  But the demands of globally scalable, always available internet applications were inconsistent with the monolithic, strictly consistent RDBMS architectures of the day.  Consequently, a variety of NoSQL distributed, “eventually consistent” systems emerged around 2010 to support the needs of a new generation of internal applications.

While these NoSQL solutions have their advantages, they are a step backward for many or most applications.  The inability to guarantee data correctness and the loss of the highly familiar and productive SQL language was a regression in many respects.   CockroachDB was designed as a highly consistent and highly available SQL-based transactional database that provides a better compromise between availability and consistency—prioritizing consistency above all but providing very high availability.

CockroachDB is a highly available, transactionally consistent SQL database compatible with existing development frameworks and with increasingly important containerized deployment models and cloud architectures. CockroachDB has been deployed at scale across a wide range of verticals and circumstances.   

In the next chapter, we’ll examine the architecture of CockroachDB and see exactly how it achieves its ambitious design goals.
 
